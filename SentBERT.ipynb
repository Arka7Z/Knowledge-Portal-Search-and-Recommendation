{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning download of datasets\n",
      "Download AllNLI.zip\n",
      "Extract AllNLI.zip\n",
      "Download stsbenchmark.zip\n",
      "Extract stsbenchmark.zip\n",
      "Download STS2017.en-de.txt.gz\n",
      "All datasets downloaded and extracted\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "folder_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "print('Beginning download of datasets')\n",
    "\n",
    "#datasets = ['AllNLI.zip', 'stsbenchmark.zip', 'wikipedia-sections-triplets.zip', 'STS2017.en-de.txt.gz', 'TED2013-en-de.txt.gz', 'xnli-en-de.txt.gz']\n",
    "datasets = ['AllNLI.zip', 'stsbenchmark.zip', 'STS2017.en-de.txt.gz']\n",
    "server = \"https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Download\", dataset)\n",
    "    url = server+dataset\n",
    "    dataset_path = os.path.join(folder_path, dataset)\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "\n",
    "    if dataset.endswith('.zip'):\n",
    "        print(\"Extract\", dataset)\n",
    "        with zipfile.ZipFile(dataset_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        os.remove(dataset_path)\n",
    "\n",
    "\n",
    "print(\"All datasets downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "model_name = 'allenai/scibert_scivocab_cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save path:  models/training_nli_allenai-scibert_scivocab_cased-2020-04-20_18-03-25\n",
      "2020-04-20 18:03:30 - Lock 140340517905296 acquired on /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696.lock\n",
      "2020-04-20 18:03:30 - https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/config.json not found in cache or force_download set to True, downloading to /home/ubuntu/.cache/torch/transformers/tmpe3ovtqwi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6016e8a3c4b34293a22d328f9882d4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=313.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-20 18:03:31 - storing https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/config.json in cache at /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696\n",
      "2020-04-20 18:03:31 - creating metadata file for /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696\n",
      "2020-04-20 18:03:31 - Lock 140340517905296 released on /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696.lock\n",
      "2020-04-20 18:03:31 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/config.json from cache at /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696\n",
      "2020-04-20 18:03:31 - Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "2020-04-20 18:03:32 - Lock 140340502619280 acquired on /home/ubuntu/.cache/torch/transformers/b79b81a602400229c8f721cd9c9147e170d4ae1695f4125badbc9d122e509839.dab672d3d9c86398f8504e0e3f46391eca25af4f7b7a88bae481e9d6974e731f.lock\n",
      "2020-04-20 18:03:32 - https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ubuntu/.cache/torch/transformers/tmp89qlmmh1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e8fe2c429e45a997f6ac4260d5042a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442301670.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-20 18:04:26 - storing https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/pytorch_model.bin in cache at /home/ubuntu/.cache/torch/transformers/b79b81a602400229c8f721cd9c9147e170d4ae1695f4125badbc9d122e509839.dab672d3d9c86398f8504e0e3f46391eca25af4f7b7a88bae481e9d6974e731f\n",
      "2020-04-20 18:04:26 - creating metadata file for /home/ubuntu/.cache/torch/transformers/b79b81a602400229c8f721cd9c9147e170d4ae1695f4125badbc9d122e509839.dab672d3d9c86398f8504e0e3f46391eca25af4f7b7a88bae481e9d6974e731f\n",
      "2020-04-20 18:04:26 - Lock 140340502619280 released on /home/ubuntu/.cache/torch/transformers/b79b81a602400229c8f721cd9c9147e170d4ae1695f4125badbc9d122e509839.dab672d3d9c86398f8504e0e3f46391eca25af4f7b7a88bae481e9d6974e731f.lock\n",
      "2020-04-20 18:04:26 - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/b79b81a602400229c8f721cd9c9147e170d4ae1695f4125badbc9d122e509839.dab672d3d9c86398f8504e0e3f46391eca25af4f7b7a88bae481e9d6974e731f\n",
      "2020-04-20 18:04:29 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/config.json from cache at /home/ubuntu/.cache/torch/transformers/560df3639836cbc0b55a7264963b1b5a7abc7ab307932944f88d56a79daf9f77.5f40512b66512e48222f7267da169e756934fb080cd4a0f6e9ba46da19ff8696\n",
      "2020-04-20 18:04:29 - Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "2020-04-20 18:04:29 - Model name 'allenai/scibert_scivocab_cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-04-20 18:04:30 - Lock 140343828292880 acquired on /home/ubuntu/.cache/torch/transformers/ae3febdd51990b429457018b5852c92e1be6bb95248faf4ac377d66fc4a5d8d0.a8e6b7905f755590e7ebfff6b58d35a8589e27e0ad0033165cb1bfd32dc9bbed.lock\n",
      "2020-04-20 18:04:30 - https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/vocab.txt not found in cache or force_download set to True, downloading to /home/ubuntu/.cache/torch/transformers/tmpv64c3w86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9788bb1c82584a2dbdfc47cfb98ce437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=222296.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-20 18:04:32 - storing https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/vocab.txt in cache at /home/ubuntu/.cache/torch/transformers/ae3febdd51990b429457018b5852c92e1be6bb95248faf4ac377d66fc4a5d8d0.a8e6b7905f755590e7ebfff6b58d35a8589e27e0ad0033165cb1bfd32dc9bbed\n",
      "2020-04-20 18:04:32 - creating metadata file for /home/ubuntu/.cache/torch/transformers/ae3febdd51990b429457018b5852c92e1be6bb95248faf4ac377d66fc4a5d8d0.a8e6b7905f755590e7ebfff6b58d35a8589e27e0ad0033165cb1bfd32dc9bbed\n",
      "2020-04-20 18:04:32 - Lock 140343828292880 released on /home/ubuntu/.cache/torch/transformers/ae3febdd51990b429457018b5852c92e1be6bb95248faf4ac377d66fc4a5d8d0.a8e6b7905f755590e7ebfff6b58d35a8589e27e0ad0033165cb1bfd32dc9bbed.lock\n",
      "2020-04-20 18:04:34 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/ae3febdd51990b429457018b5852c92e1be6bb95248faf4ac377d66fc4a5d8d0.a8e6b7905f755590e7ebfff6b58d35a8589e27e0ad0033165cb1bfd32dc9bbed\n",
      "2020-04-20 18:04:34 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/added_tokens.json from cache at None\n",
      "2020-04-20 18:04:34 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/special_tokens_map.json from cache at None\n",
      "2020-04-20 18:04:34 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_cased/tokenizer_config.json from cache at None\n",
      "2020-04-20 18:04:34 - Use pytorch device: cpu\n",
      "2020-04-20 18:04:34 - Read AllNLI train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 942069/942069 [12:13<00:00, 1284.35it/s]\n",
      "Convert dataset:  15%|█▌        | 228/1500 [00:00<00:00, 2270.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-20 18:16:57 - Num sentences: 942069\n",
      "2020-04-20 18:16:57 - Sentences 0 longer than max_seqence_length: 987\n",
      "2020-04-20 18:16:57 - Sentences 1 longer than max_seqence_length: 0\n",
      "2020-04-20 18:16:57 - Softmax loss: #Vectors concatenated: 3\n",
      "2020-04-20 18:16:57 - Read STSbenchmark dev dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 1500/1500 [00:01<00:00, 1275.44it/s]\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/58880 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-20 18:16:58 - Num sentences: 1500\n",
      "2020-04-20 18:16:58 - Sentences 0 longer than max_seqence_length: 0\n",
      "2020-04-20 18:16:58 - Sentences 1 longer than max_seqence_length: 0\n",
      "2020-04-20 18:16:58 - Warmup-steps: 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 1/58880 [00:30<497:15:12, 30.40s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/58880 [01:32<522:57:57, 31.98s/it]\u001b[A\n",
      "Iteration:   0%|          | 3/58880 [02:09<527:20:50, 32.24s/it]\u001b[A\n",
      "Iteration:   0%|          | 4/58880 [02:49<533:36:38, 32.63s/it]\u001b[A\n",
      "Iteration:   0%|          | 5/58880 [03:29<539:55:07, 33.01s/it]\u001b[A\n",
      "Iteration:   0%|          | 6/58880 [04:06<542:34:25, 33.18s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/58880 [04:42<545:28:13, 33.35s/it]\u001b[A\n",
      "Iteration:   0%|          | 8/58880 [05:04<535:38:30, 32.75s/it]\u001b[A\n",
      "Iteration:   0%|          | 9/58880 [05:23<524:23:19, 32.07s/it]\u001b[A\n",
      "Iteration:   0%|          | 10/58880 [05:46<516:51:54, 31.61s/it]\u001b[A\n",
      "Iteration:   0%|          | 11/58880 [06:04<506:09:05, 30.95s/it]\u001b[A\n",
      "Iteration:   0%|          | 12/58880 [06:30<502:23:07, 30.72s/it]\u001b[A\n",
      "Iteration:   0%|          | 13/58880 [06:50<492:57:28, 30.15s/it]\u001b[A\n",
      "Iteration:   0%|          | 14/58880 [07:14<488:00:23, 29.84s/it]\u001b[A\n",
      "Iteration:   0%|          | 15/58880 [07:36<481:35:56, 29.45s/it]\u001b[A\n",
      "Iteration:   0%|          | 16/58880 [08:01<477:49:39, 29.22s/it]\u001b[A\n",
      "Iteration:   0%|          | 17/58880 [08:17<467:35:27, 28.60s/it]\u001b[A\n",
      "Iteration:   0%|          | 18/58880 [08:40<462:39:21, 28.30s/it]\u001b[A\n",
      "Iteration:   0%|          | 19/58880 [09:02<457:50:17, 28.00s/it]\u001b[A\n",
      "Iteration:   0%|          | 20/58880 [09:38<464:19:16, 28.40s/it]\u001b[A\n",
      "Iteration:   0%|          | 21/58880 [10:09<466:26:10, 28.53s/it]\u001b[A\n",
      "Iteration:   0%|          | 22/58880 [10:32<461:59:27, 28.26s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6de354d9dae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m           )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, fp16, fp16_opt_level, local_rank)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "batch_size = 16\n",
    "nli_reader = NLIDataReader('./datasets/AllNLI')\n",
    "sts_reader = STSBenchmarkDataReader('./datasets/stsbenchmark')\n",
    "train_num_labels = nli_reader.get_num_labels()\n",
    "model_save_path = 'models/training_nli_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print('model save path: ', model_save_path)\n",
    "\n",
    "\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "train_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "\n",
    "\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "\n",
    "# Configure the training\n",
    "num_epochs = 1\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_data = SentencesDataset(examples=sts_reader.get_examples(\"sts-test.csv\"), model=model)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)\n",
    "\n",
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning on sts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-nli-mean-tokens'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'models/fineTunedSciBERT\n",
    "sts_reader = STSBenchmarkDataReader('./datasets/stsbenchmark', normalize_scores=True)\n",
    "\n",
    "# Load a pre-trained sentence transformer model<- loaded in last cell for evaluation, else reload\n",
    "#model = SentenceTransformer(model_name)\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "train_data = SentencesDataset(sts_reader.get_examples('sts-train.csv'), model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_data)*num_epochs/train_batch_size*0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_data = SentencesDataset(examples=sts_reader.get_examples(\"sts-test.csv\"), model=model)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=train_batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)\n",
    "model.evaluate(evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
