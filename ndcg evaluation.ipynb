{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['converting text to speech', 'big data', 'efficient estimation of word representations in vector space', 'natural language interface', 'reinforcement learning in video game']\n",
    "queryToIdx = {queryList[i]:i for i in range(len(queryList))}\n",
    "queryToIdx['converting word to speech'] = 0\n",
    "queryToIdx['Big data'] = 1\n",
    "annotationDict = [{} for i in range(len(queryList))]\n",
    "annotationFile = './data/entityAnnotations2.csv'\n",
    "with open (annotationFile,'r') as csv_file:\n",
    "    reader =csv.reader(csv_file)\n",
    "    next(reader) # skip first row\n",
    "    for row in reader:\n",
    "        annotationDict[queryToIdx[row[0]]][row[1]] = int(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevanceScores, k = 10, method=0):\n",
    "    \"\"\"\n",
    "    Returns discounted cumulative gain (dcg)\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    relevanceScores = np.asfarray(relevanceScores)[:k]\n",
    "    if relevanceScores.size:\n",
    "        if method == 0:\n",
    "            return relevanceScores[0] + np.sum(relevanceScores[1:] / np.log2(np.arange(2, relevanceScores.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0\n",
    "\n",
    "def ndcgMax(relevanceScores, k=10, method=0):\n",
    "    return dcg(sorted(relevanceScores, reverse=True), k, method)\n",
    "\n",
    "def ndcg(relevanceScores, ndcgMax, k = 10, method=0):\n",
    "    return dcg(relevanceScores, k, method) / ndcgMax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevanceScores= []\n",
    "### search_results.json 5, entity_search_results.json 6, entity_search_resultsSiamese.json 1\n",
    "filePath = './data/'\n",
    "\n",
    "fileName = 'entity_search_results.json'\n",
    "annotationFile = filePath + fileName\n",
    "\n",
    "with open(annotationFile, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        result = data['result']\n",
    "        for i in range(len(result)):\n",
    "            result[i] = [annotationDict[i].get(ID, 0) for ID in result[i]]\n",
    "        relevanceScores.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.763483535311373, 15.763483535311373, 15.763483535311373, 15.763483535311373, 15.763483535311373]\n"
     ]
    }
   ],
   "source": [
    "ndcgMaxPerQuery = []\n",
    "for i in range(len(queryList)):\n",
    "    ndcgMaxPerQuery.append(ndcgMax(list(annotationDict[i].values())))\n",
    "ndcgMaxPerQuery[-1] = ndcgMaxPerQuery[0]\n",
    "print(ndcgMaxPerQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.763483535311373, 15.763483535311373, 15.763483535311373, 15.763483535311373, 15.763483535311373]\n"
     ]
    }
   ],
   "source": [
    "ndcgMaxPerQuery = [15.763483535311373] * 5\n",
    "print(ndcgMaxPerQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search methods: [searchWithEmbedding title USE, searchWithEmbedding fos FT, elasticSearch, rankedElasticSearch, rankedMLTElasticSearch] \n",
    "#numSearchMethods = 6\n",
    "#assert numSearchMethods == len(relevanceScores)\n",
    "results = []\n",
    "meanScores = []\n",
    "\n",
    "for i in range(len(relevanceScores)):\n",
    "    relScoresForThisMethod = relevanceScores[i]\n",
    "    ndcgScoresMethod = []\n",
    "    for q in range(len(queryList)):\n",
    "        ndcgScoresMethod.append(ndcg(relScoresForThisMethod[q], ndcgMaxPerQuery[q]))\n",
    "    results.append(ndcgScoresMethod)\n",
    "    meanScores.append(np.mean(ndcgScoresMethod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9762365622836594,\n",
       " 0.9087934022664783,\n",
       " 0.9555935717868485,\n",
       " 0.8085255869022709,\n",
       " 0.9336545127057235,\n",
       " 0.9403742034003058]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93, 1.0, 1.0, 1.0, 0.95]\n",
      "[0.84, 1.0, 0.72, 1.0, 0.98]\n",
      "[0.94, 1.0, 1.0, 1.0, 0.84]\n",
      "[0.32, 1.0, 0.97, 1.0, 0.75]\n",
      "[0.74, 1.0, 0.98, 1.0, 0.95]\n",
      "[0.81, 1.0, 0.98, 1.0, 0.91]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    result = [round(i, 2) for i in result]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e2b0bb0e8e4e43bfe7e7a53d019146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'])\n",
    "        IDList.append(data['id'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'entity_search_results.json'\n",
    "annotationFile = filePath + fileName\n",
    "\n",
    "with open(annotationFile, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        result = data['result']\n",
    "        for i in range(len(result)):\n",
    "            result[i] = [ret(ID) for ID in result[i]]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Secure Mobile Crowdsensing Game With Deep Reinforcement Learning',\n",
       " 'Automatic computer game balancing: a reinforcement learning approach',\n",
       " 'Application of reinforcement learning to the game of Othello',\n",
       " 'Position-based reinforcement learning biased MCTS for General Video Game Playing',\n",
       " 'EXPERIMENTS WITH ONLINE REINFORCEMENT LEARNING IN REAL-TIME STRATEGY GAMES',\n",
       " 'An object-oriented approach to reinforcement learning in an action game',\n",
       " 'Deep Learning for Video Game Playing.',\n",
       " 'High-level reinforcement learning in strategy games',\n",
       " 'GENERAL GAME-PLAYING AND REINFORCEMENT LEARNING',\n",
       " 'A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [\"2744714095\", \"2042439732\", \"2016945372\", \"2590481545\", \"2017980908\", \"2191395120\", \"2753316839\", \"1584307643\", \"2099100618\", \"2750605955\"]\n",
    "[ret(ID) for ID in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcg([3, 3, 3, 3, 3, 3, 0, 3, 2, 3]) / ndcgMaxPerQuery[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'entity_search_results.json'\n",
    "annotationFile = filePath + fileName\n",
    "with open(annotationFile, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        result = data['result']\n",
    "        for i in range(len(result)):\n",
    "            result[i] = [annotationDict[i].get(ID, 0) for ID in result[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotationDict[4]['2166159790']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['Novel-word pronunciation within a text-to-speech system', 'Big Data Framework', 'Efficient Estimation of Word Representations in Vector Space', 'Natural Language Interface Using Shallow Parsing.', 'Reinforcement Learning in First Person Shooter Games']\n",
    "queryToIdx = {queryList[i]:i for i in range(len(queryList))}\n",
    "annotationDict = [{} for i in range(len(queryList))]\n",
    "\n",
    "with open ('./data/recommendationAnnotations2.csv','r') as csv_file:\n",
    "    reader =csv.reader(csv_file)\n",
    "    next(reader) # skip first row\n",
    "    for row in reader:\n",
    "        annotationDict[queryToIdx[row[0]]][row[1]] = row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevanceScores= []\n",
    "fileName = './data/recommendation_results2.json'\n",
    "with open(fileName, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        result = data['result']\n",
    "        for i in range(len(result)):\n",
    "            result[i] = [annotationDict[i].get(ID, 0) for ID in result[i]]\n",
    "        relevanceScores.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgMaxPerQuery = []\n",
    "for i in range(len(queryList)):\n",
    "    ndcgMaxPerQuery.append(ndcgMax(list(annotationDict[i].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search methods: [MLT, K closest node embed direct, node rerankedcosine USEabstract, node rerankedExemplar USEabstract\n",
    "##                 node rerankedcosine TfIdf, node rerankedExemplar TfIdf] \n",
    "numSearchMethods = 3\n",
    "assert numSearchMethods == len(relevanceScores)\n",
    "results = []\n",
    "meanScores = []\n",
    "\n",
    "for i in range(numSearchMethods):\n",
    "    relScoresForThisMethod = relevanceScores[i]\n",
    "    ndcgScoresMethod = []\n",
    "    for q in range(len(queryList)):\n",
    "        ndcgScoresMethod.append(ndcg(relScoresForThisMethod[q], ndcgMaxPerQuery[q]))\n",
    "    results.append(ndcgScoresMethod)\n",
    "    meanScores.append(np.mean(ndcgScoresMethod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.635, 0.746, 0.734]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(score, 3) for score in meanScores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.605, 1.0, 0.629, 0.285, 0.655]\n",
      "[0.758, 1.0, 0.899, 0.352, 0.72]\n",
      "[0.705, 1.0, 0.868, 0.281, 0.819]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    result = [round(i, 3) for i in result]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcg([0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3] , 10) / 15.763483535311373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcg([0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3] , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.763483535311373,\n",
       " 15.763483535311373,\n",
       " 15.763483535311373,\n",
       " 15.763483535311373,\n",
       " 15.763483535311373]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcgMaxPerQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating topic suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['chatbot', 'heuristic search', 'cnn', 'word embedding', 'activation function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = [[1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1,], [1, 1, 1, 1], [0, 1, 1, 1]]\n",
    "ss = [[0, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 0]]\n",
    "arxlive = [[0, 0, 1, 0], [0, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgMaxPerQuery = [ndcgMax([1] * 4)] * 5\n",
    "relevanceScores = [ours, ss, arxlive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "meanScores = []\n",
    "numSearchMethods = 3\n",
    "for i in range(numSearchMethods):\n",
    "    relScoresForThisMethod = relevanceScores[i]\n",
    "    ndcgScoresMethod = []\n",
    "    for q in range(len(queryList)):\n",
    "        ndcgScoresMethod.append(ndcg(relScoresForThisMethod[q], ndcgMaxPerQuery[q]))\n",
    "    results.append(ndcgScoresMethod)\n",
    "    meanScores.append(np.mean(ndcgScoresMethod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9, 0.14, 0.18]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(score, 2) for score in meanScores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84, 1.0, 1.0, 1.0, 0.681]\n",
      "[0.0, 0.479, 0.0, 0.202, 0.0]\n",
      "[0.202, 0.0, 0.202, 0.319, 0.202]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    result = [round(i, 3) for i in result]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
