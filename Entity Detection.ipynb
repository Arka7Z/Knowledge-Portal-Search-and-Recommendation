{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_proxy  = \"http://172.16.2.30:8080\"\n",
    "https_proxy = \"http://172.16.2.30:8080\"\n",
    "ftp_proxy   = \"http://172.16.2.30:8080\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "              \"ftp\"   : ftp_proxy\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getEntitiesAndSpots(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'true', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/papersForEntity.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "embeddingResults = data['embeddingResults']\n",
    "esResults = data['esResults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperIDs = set()\n",
    "for result in embeddingResults:\n",
    "    paperIDs.update(result)\n",
    "for result in esResults:\n",
    "    paperIDs.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = dict()\n",
    "PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "i = 0\n",
    "\n",
    "with open(PapersOutFileName, 'r') as file:\n",
    "    for line in file:\n",
    "        if i % 2 != 0:\n",
    "            data = json.loads(line)\n",
    "            if (data['id'] in paperIDs):\n",
    "                records[data['id']] = {'title': data['title'], 'abstract': data['abstract']}\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac539f3d5f3a43ff8e4f79959c3e88cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=430.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictForBody = dict() ## dict[paperId] = {'entities': entityCounterList,'spots': spotscounterList}\n",
    "for key in tqdm(records.keys()):\n",
    "    text = records[key]['title'] + ' ' + records[key]['abstract']\n",
    "    spots, entities = getEntitiesAndSpots(text, rhoThreshold = 0.12, long_text = 5)\n",
    "    dictForBody[key] = {'entities': entities, 'spots': spots}\n",
    "with open('./data/BodyEntitiesPerPaper.json', 'w') as outfile:\n",
    "    json.dump(dictForBody, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544c936ed33c45a7aeb8bce86ba73dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=430.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dictForTitles = dict() ## dict[paperId] = {'entities': entityCounterList,'spots': spotscounterList}\n",
    "for key in tqdm(records.keys()):\n",
    "    text = records[key]['title']\n",
    "    spots, entities = getEntitiesAndSpots(text, rhoThreshold = 0.1)\n",
    "    dictForTitles[key] = {'entities': entities, 'spots': spots}\n",
    "with open('./data/TitleEntitiesPerPaper.json', 'w') as outfile:\n",
    "    json.dump(dictForTitles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dictForTitles = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityList = [[entityTuple[0] for entityTuple in tmpDict['entities']] for tmpDict in  dictForTitles.values() ]\n",
    "entitySet = set()\n",
    "for entitySubList in entityList:\n",
    "    entitySet.update(entitySubList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dictForBody = json.loads(line)\n",
    "entityList = [[entityTuple[0] for entityTuple in tmpDict['entities']] for tmpDict in  dictForBody.values() ]\n",
    "for entitySubList in entityList:\n",
    "    entitySet.update(entitySubList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entitySet)\n",
    "entityList = list(entitySet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100     # block size\n",
    "entities = [entityList[i:i + n] for i in range(0, len(entityList), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3bd8eb10ca4873909e2b4159f11ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./data/entity_USE_Embeddings.json', 'w') as outfile:\n",
    "    for entitySubList in tqdm(entities):\n",
    "        entitySubList = [preprocess(entity) for entity in entitySubList]\n",
    "        embeddings = embed(entitySubList).numpy().tolist()\n",
    "        for embedding, entity in zip(embeddings, entitySubList):\n",
    "            outDict = dict()\n",
    "            outDict['entity'] = entity\n",
    "            outDict['embedding'] = embedding\n",
    "            count += 1\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')\n",
    "assert count = len(entityList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoS processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11357da3173647c48e8af528c51d2cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "fosList = []\n",
    "fosCount = defaultdict(int)\n",
    "\n",
    "for record in tqdm(records):    \n",
    "    for fos in record['fos']:\n",
    "        fosCount[fos] += 1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosList = [(count, tag) for tag, count in fosCount.items()]\n",
    "fosList.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagIdf =  [(len(records) / count , tag) for tag, count in fosCount.items() if count > 50]\n",
    "tagIdf.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35325"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fosCount.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate FastText embeddings for the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import fasttext \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "fasttextModel = fasttext.load_model('crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e223897fad6745d09caef5a4953a9067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/dblp_fos_FT_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        embedding = fasttextModel.get_word_vector(fos).tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34721ded247a409f831345d3a307f853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "with open('./data/dblp_fos_FT_Phrase_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        fos.strip()\n",
    "        fos = fos.translate(translator)\n",
    "        fos = '_'.join(fos.split())\n",
    "        embedding = fasttextModel.get_word_vector(fos).tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d0c9131776477aa430e6e879ff8486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "with open('./data/dblp_fos_USE_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        fos.strip()\n",
    "        fos = fos.translate(translator)\n",
    "        fos = ' '.join(fos.split())\n",
    "        embedding = embed([fos])[0].numpy().tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88841444"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarity(fasttextModel.get_word_vector('natural language user interface'),fasttextModel.get_word_vector('natural language interface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81671053"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarity(fasttextModel.get_word_vector('natural language'),fasttextModel.get_word_vector('natural language user interface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1129     1  chosen guideline ontology\n",
      "[chosen guideline ontology]\n",
      "0.1114     1  text formal representation\n",
      "[text formal representation]\n",
      "0.1033     1  semi-structured text\n",
      "[semi-structured text]\n",
      "0.1032     1  asbru gem ontologies\n",
      "[Asbru GEM ontologies]\n",
      "0.0934     2  digital electronic guideline library\n",
      "[Digital Electronic Guideline Library, Digital Electronic Guideline Library]\n",
      "0.0824     1  major tool\n",
      "[major tool]\n",
      "0.0755     2  representation\n",
      "[representation, representations]\n",
      "0.0704     1  semantic markup\n",
      "[semantic markup]\n",
      "0.0676     1  clinicians\n",
      "[clinicians]\n",
      "0.0672     1  retrospective assessment\n",
      "[retrospective assessment]\n",
      "0.0670     1  gradual conversion\n",
      "[gradual conversion]\n",
      "0.0652     1  medical care\n",
      "[medical care]\n",
      "0.0647     1  clinical guidelines\n",
      "[Clinical Guidelines]\n",
      "0.0630     1  demonstrated feasibility\n",
      "[demonstrated feasibility]\n",
      "0.0622     1  domain knowledge\n",
      "[domain knowledge]\n",
      "0.0617     1  hybrid meta-ontology\n",
      "[hybrid meta-ontology]\n",
      "0.0582     1  asbru\n",
      "[Asbru]\n",
      "0.0566     1  classification\n",
      "[classification]\n",
      "0.0545     1  run-time application\n",
      "[run-time application]\n",
      "0.0544     1  browsing\n",
      "[browsing]\n",
      "0.0535     1  elements\n",
      "[elements]\n",
      "0.0521     1  context-sensitive search\n",
      "[context-sensitive search]\n",
      "0.0499     1  all ontologies\n",
      "[all ontologies]\n",
      "0.0491     1  point\n",
      "[point]\n",
      "0.0476     1  increasingly sophisticated computational tasks\n",
      "[increasingly sophisticated computational tasks]\n",
      "0.0436     1  web-based, modular, distributed architecture\n",
      "[Web-based, modular, distributed architecture]\n",
      "0.0435     2  degel\n",
      "[DeGeL, DeGeL]\n",
      "0.0365     1  also three guideline-content formats\n",
      "[also three guideline-content formats]\n",
      "0.0364     1  the architecture\n",
      "[The architecture]\n",
      "0.0348     1  all tools\n",
      "[All tools]\n",
      "0.0339     1  these formats\n",
      "[These formats]\n",
      "0.0312     1  the quality\n",
      "[the quality]\n",
      "0.0000     2  we\n",
      "[We, three]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = records[10109]['abstract']\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"hello\"\n",
    "a = b\n",
    "b = \"efg\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
