{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_proxy  = \"http://172.16.2.30:8080\"\n",
    "https_proxy = \"http://172.16.2.30:8080\"\n",
    "ftp_proxy   = \"http://172.16.2.30:8080\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "              \"ftp\"   : ftp_proxy\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module provides a wrapper for the TagMe API.\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import dateutil.parser\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import six\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "__all__ = [\n",
    "    'annotate', 'mentions', 'relatedness_wid', 'relatedness_title', 'Annotation',\n",
    "    'AnnotateResponse', 'Mention', 'MentionsResponse', 'Relatedness', 'RelatednessResponse',\n",
    "    'normalize_title', 'title_to_uri',\n",
    "    ]\n",
    "\n",
    "__author__ = 'Marco Cornolti <cornolti@di.unipi.it>'\n",
    "\n",
    "DEFAULT_TAG_API = \"https://tagme.d4science.org/tagme/tag\"\n",
    "DEFAULT_SPOT_API = \"https://tagme.d4science.org/tagme/spot\"\n",
    "DEFAULT_REL_API = \"https://tagme.d4science.org/tagme/rel\"\n",
    "DEFAULT_LANG = \"en\"\n",
    "DEFAULT_LONG_TEXT = 3\n",
    "WIKIPEDIA_URI_BASE = u\"https://{}.wikipedia.org/wiki/{}\"\n",
    "MAX_RELATEDNESS_PAIRS_PER_REQUEST = 100\n",
    "GCUBE_TOKEN = None\n",
    "HTML_PARSER = HTMLParser()\n",
    "\n",
    "class Annotation(object):\n",
    "    '''\n",
    "    An annotation, i.e. a link of a part of text to an entity.\n",
    "    '''\n",
    "    def __init__(self, ann_json):\n",
    "        self.begin = int(ann_json.get(\"start\"))\n",
    "        self.end = int(ann_json.get(\"end\"))\n",
    "        self.entity_id = int(ann_json.get(\"id\"))\n",
    "        self.entity_title = ann_json.get(\"title\")\n",
    "        self.score = float(ann_json.get(\"rho\"))\n",
    "        self.mention = ann_json.get(\"spot\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return u\"{} -> {} (score: {})\".format(self.mention, self.entity_title, self.score)\n",
    "\n",
    "    def uri(self, lang=DEFAULT_LANG):\n",
    "        '''\n",
    "        Get the URI of this annotation entity.\n",
    "        :param lang: the Wikipedia language.\n",
    "        '''\n",
    "        return title_to_uri(self.entity_title, lang)\n",
    "\n",
    "\n",
    "class AnnotateResponse(object):\n",
    "    '''\n",
    "    A response to a call to the annotation (/tag) service. It contains the list of annotations\n",
    "    found.\n",
    "    '''\n",
    "    def __init__(self, json_content):\n",
    "        self.annotations = [Annotation(ann_json) for ann_json in json_content[\"annotations\"] if \"title\" in ann_json]\n",
    "        self.time = int(json_content[\"time\"])\n",
    "        self.lang = json_content[\"lang\"]\n",
    "        self.timestamp = dateutil.parser.parse(json_content[\"timestamp\"])\n",
    "        self.original_json = json_content\n",
    "\n",
    "    def get_annotations(self, min_rho=None):\n",
    "        '''\n",
    "        Get the list of annotations found.\n",
    "        :param min_rho: if set, only get entities with a rho-score (confidence) higher than this.\n",
    "        '''\n",
    "        return (a for a in self.annotations if min_rho is None or a.score > min_rho)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{}msec, {} annotations\".format(self.time, len(self.annotations))\n",
    "\n",
    "\n",
    "class Mention(object):\n",
    "    '''\n",
    "    A mention, i.e. a part of text that may mention an entity.\n",
    "    '''\n",
    "    def __init__(self, mention_json):\n",
    "        self.begin = int(mention_json.get(\"start\"))\n",
    "        self.end = int(mention_json.get(\"end\"))\n",
    "        self.linkprob = float(mention_json.get(\"lp\"))\n",
    "        self.mention = mention_json.get(\"spot\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return u\"{} [{},{}] lp={}\".format(self.mention, self.begin, self.end, self.linkprob)\n",
    "\n",
    "\n",
    "class MentionsResponse(object):\n",
    "    '''\n",
    "    A response to a call to the mention finding (/spot) service. It contains the list of mentions\n",
    "    found.\n",
    "    '''\n",
    "    def __init__(self, json_content):\n",
    "        self.mentions = [Mention(mention_json) for mention_json in json_content[\"spots\"]]\n",
    "        self.time = int(json_content[\"time\"])\n",
    "        self.lang = json_content[\"lang\"]\n",
    "        self.timestamp = dateutil.parser.parse(json_content[\"timestamp\"])\n",
    "\n",
    "    def get_mentions(self, min_lp=None):\n",
    "        '''\n",
    "        Get the list of mentions found.\n",
    "        :param min_lp: if set, only get mentions with a link probability higher than this.\n",
    "        '''\n",
    "        return (m for m in self.mentions if min_lp is None or m.linkprob > min_lp)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{}msec, {} mentions\".format(self.time, len(self.mentions))\n",
    "\n",
    "\n",
    "class Relatedness(object):\n",
    "    '''\n",
    "    A relatedness, i.e. a real value between 0 and 1 indicating how semantically close two entities\n",
    "    are.\n",
    "    '''\n",
    "    def __init__(self, rel_json):\n",
    "        self.title1, self.title2 = (wiki_title(t) for t in rel_json[\"couple\"].split(\" \"))\n",
    "        self.rel = float(rel_json[\"rel\"]) if \"rel\" in rel_json else None\n",
    "\n",
    "    def as_pair(self):\n",
    "        '''\n",
    "        Get this relatedness value as a pair (titles, rel), where rel is the relatedness value and\n",
    "        titles is the pair of the two titles/Wikipedia IDs.\n",
    "        '''\n",
    "        return ((self.title1, self.title2), self.rel)\n",
    "\n",
    "    def __str__(self):\n",
    "        return u\"{}, {} rel={}\".format(self.title1, self.title2, self.rel)\n",
    "\n",
    "\n",
    "class RelatednessResponse(object):\n",
    "    '''\n",
    "    A response to a call to the relatedness (/rel) service. It contains the list of relatedness for\n",
    "    each pair.\n",
    "    '''\n",
    "    def __init__(self, json_contents):\n",
    "        self.relatedness = [Relatedness(rel_json)\n",
    "                            for json_content in json_contents\n",
    "                            for rel_json in json_content[\"result\"]]\n",
    "        self.lang = json_contents[0][\"lang\"]\n",
    "        self.timestamp = dateutil.parser.parse(json_contents[0][\"timestamp\"])\n",
    "        self.calls = len(json_contents)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for rel in self.relatedness:\n",
    "            yield rel.as_pair()\n",
    "\n",
    "    def get_relatedness(self, i=0):\n",
    "        '''\n",
    "        Get the relatedness of a pairs of entities.\n",
    "        :param i: the index of an entity pair. The order is the same as the request.\n",
    "        '''\n",
    "        return self.relatedness[i].rel\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{} relatedness pairs, {} calls\".format(len(self.relatedness), self.calls)\n",
    "\n",
    "\n",
    "def normalize_title(title):\n",
    "    '''\n",
    "    Normalize a title to Wikipedia format. E.g. \"barack Obama\" becomes \"Barack_Obama\"\n",
    "    :param title: a title to normalize.\n",
    "    '''\n",
    "    title = title.strip().replace(\" \", \"_\")\n",
    "    return title[0].upper() + title[1:]\n",
    "\n",
    "\n",
    "def wiki_title(title):\n",
    "    '''\n",
    "    Given a normalized title, get the page title. E.g. \"Barack_Obama\" becomes \"Barack Obama\"\n",
    "    :param title: a wikipedia title.\n",
    "    '''\n",
    "    return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
    "\n",
    "\n",
    "def title_to_uri(entity_title, lang=DEFAULT_LANG):\n",
    "    '''\n",
    "    Get the URI of the page describing a Wikipedia entity.\n",
    "    :param entity_title: an entity title.\n",
    "    :param lang: the Wikipedia language.\n",
    "    '''\n",
    "    return WIKIPEDIA_URI_BASE.format(lang, normalize_title(entity_title))\n",
    "\n",
    "\n",
    "def annotate(text, gcube_token=None, lang=DEFAULT_LANG, api=DEFAULT_TAG_API,\n",
    "             long_text=DEFAULT_LONG_TEXT):\n",
    "    '''\n",
    "    Annotate a text, linking it to Wikipedia entities.\n",
    "    :param text: the text to annotate.\n",
    "    :param gcube_token: the authentication token provided by the D4Science infrastructure.\n",
    "    :param lang: the Wikipedia language.\n",
    "    :param api: the API endpoint.\n",
    "    :param long_text: long_text parameter (see TagMe documentation).\n",
    "    '''\n",
    "    payload = [(\"text\", text.encode(\"utf-8\")),\n",
    "               (\"long_text\", long_text),\n",
    "               (\"lang\", lang)]\n",
    "    json_response = _issue_request(api, payload, gcube_token)\n",
    "    return AnnotateResponse(json_response) if json_response else None\n",
    "\n",
    "\n",
    "def mentions(text, gcube_token=None, lang=DEFAULT_LANG, api=DEFAULT_SPOT_API):\n",
    "    '''\n",
    "    Find possible mentions in a text, do not link them to any entity.\n",
    "    :param text: the text where to find mentions.\n",
    "    :param gcube_token: the authentication token provided by the D4Science infrastructure.\n",
    "    :param lang: the Wikipedia language.\n",
    "    :param api: the API endpoint.\n",
    "    '''\n",
    "    payload = [(\"text\", text.encode(\"utf-8\")),\n",
    "               (\"lang\", lang.encode(\"utf-8\"))]\n",
    "    json_response = _issue_request(api, payload, gcube_token)\n",
    "    return MentionsResponse(json_response) if json_response else None\n",
    "\n",
    "\n",
    "def relatedness_wid(wid_pairs, gcube_token=None, lang=DEFAULT_LANG, api=DEFAULT_REL_API):\n",
    "    '''\n",
    "    Get the semantic relatedness among pairs of entities. Entities are indicated by their\n",
    "    Wikipedia ID (an integer).\n",
    "    :param wid_pairs: either one pair or a list of pairs of Wikipedia IDs.\n",
    "    :param gcube_token: the authentication token provided by the D4Science infrastructure.\n",
    "    :param lang: the Wikipedia language.\n",
    "    :param api: the API endpoint.\n",
    "    '''\n",
    "    return _relatedness(\"id\", wid_pairs, gcube_token, lang, api)\n",
    "\n",
    "\n",
    "def relatedness_title(tt_pairs, gcube_token=None, lang=DEFAULT_LANG, api=DEFAULT_REL_API):\n",
    "    '''\n",
    "    Get the semantic relatedness among pairs of entities. Entities are indicated by their\n",
    "    Wikipedia ID (an integer).\n",
    "    :param tt_pairs: either one pair or a list of pairs of entity titles.\n",
    "    :param gcube_token: the authentication token provided by the D4Science infrastructure.\n",
    "    :param lang: the Wikipedia language.\n",
    "    :param api: the API endpoint.\n",
    "    '''\n",
    "    return _relatedness(\"tt\", tt_pairs, gcube_token, lang, api)\n",
    "\n",
    "\n",
    "def _relatedness(pairs_type, pairs, gcube_token, lang, api):\n",
    "    if not isinstance(pairs[0], (list, tuple)):\n",
    "        pairs = [pairs]\n",
    "\n",
    "    if isinstance(pairs[0][0], six.binary_type):  # str in python 2, bytes in python 3\n",
    "        pairs = [(p[0].decode(\"utf-8\"), p[1].decode(\"utf-8\")) for p in pairs]\n",
    "\n",
    "    if isinstance(pairs[0][0], six.text_type):  # unicode in python 2, str in python 3\n",
    "        pairs = [(normalize_title(p[0]), normalize_title(p[1])) for p in pairs]\n",
    "\n",
    "    json_responses = []\n",
    "    for chunk in range(0, len(pairs), MAX_RELATEDNESS_PAIRS_PER_REQUEST):\n",
    "        payload = [(\"lang\", lang)]\n",
    "        payload += ((pairs_type, u\"{} {}\".format(p[0], p[1]))\n",
    "                    for p in pairs[chunk:chunk + MAX_RELATEDNESS_PAIRS_PER_REQUEST])\n",
    "        json_responses.append(_issue_request(api, payload, gcube_token))\n",
    "    return RelatednessResponse(json_responses) if json_responses and json_responses[0] else None\n",
    "\n",
    "\n",
    "def _issue_request(api, payload, gcube_token):\n",
    "    if not gcube_token:\n",
    "        gcube_token = GCUBE_TOKEN\n",
    "    if not gcube_token:\n",
    "        raise RuntimeError(\"You must define GCUBE_TOKEN before calling this function or pass the \"\n",
    "                           \"gcube_token parameter.\")\n",
    "\n",
    "    payload.append((\"gcube-token\", gcube_token))\n",
    "    logging.debug(\"Calling %s\", api)\n",
    "    res = requests.post(api, data=payload,proxies=proxyDict)\n",
    "    if res.status_code != 200:\n",
    "        logging.warning(\"Tagme returned status code %d message:\\n%s\", res.status_code, res.content)\n",
    "        return None\n",
    "    res_content = res.content.decode(\"utf-8\") if isinstance(res.content, six.binary_type) else res.content\n",
    "    return json.loads(res_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess('natural language interface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tagme.d4science.org/tagme/tag'\n",
    "params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'true', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_proxy  = \"http://172.16.2.30:8080\"\n",
    "https_proxy = \"http://172.16.2.30:8080\"\n",
    "ftp_proxy   = \"http://172.16.2.30:8080\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "              \"ftp\"   : ftp_proxy\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475839"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "i = 0\n",
    "\n",
    "with open(PapersOutFileName, 'r') as file:\n",
    "    for line in file:\n",
    "        if i % 2 != 0:\n",
    "            data = json.loads(line)\n",
    "            records.append(data)\n",
    "        i += 1\n",
    "\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11357da3173647c48e8af528c51d2cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "fosList = []\n",
    "fosCount = defaultdict(int)\n",
    "\n",
    "for record in tqdm(records):    \n",
    "    for fos in record['fos']:\n",
    "        fosCount[fos] += 1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosList = [(count, tag) for tag, count in fosCount.items()]\n",
    "fosList.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagIdf =  [(len(records) / count , tag) for tag, count in fosCount.items() if count > 50]\n",
    "tagIdf.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35325"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fosCount.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate FastText embeddings for the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import fasttext \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "fasttextModel = fasttext.load_model('crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e223897fad6745d09caef5a4953a9067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/dblp_fos_FT_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        embedding = fasttextModel.get_word_vector(fos).tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34721ded247a409f831345d3a307f853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "with open('./data/dblp_fos_FT_Phrase_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        fos.strip()\n",
    "        fos = fos.translate(translator)\n",
    "        fos = '_'.join(fos.split())\n",
    "        embedding = fasttextModel.get_word_vector(fos).tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d0c9131776477aa430e6e879ff8486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35325.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "with open('./data/dblp_fos_USE_embeddings.json', 'w') as outfile:\n",
    "    for count, fos in tqdm(fosList):\n",
    "        orgFos = fos\n",
    "        fos.strip()\n",
    "        fos = fos.translate(translator)\n",
    "        fos = ' '.join(fos.split())\n",
    "        embedding = embed([fos])[0].numpy().tolist()    # while reading use np.asarray to convert to np array\n",
    "        outDict = dict()\n",
    "        outDict['fos'] = orgFos\n",
    "        outDict['embedding'] = embedding\n",
    "        outDict['count'] = count\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88841444"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarity(fasttextModel.get_word_vector('natural language user interface'),fasttextModel.get_word_vector('natural language interface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81671053"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarity(fasttextModel.get_word_vector('natural language'),fasttextModel.get_word_vector('natural language user interface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1129     1  chosen guideline ontology\n",
      "[chosen guideline ontology]\n",
      "0.1114     1  text formal representation\n",
      "[text formal representation]\n",
      "0.1033     1  semi-structured text\n",
      "[semi-structured text]\n",
      "0.1032     1  asbru gem ontologies\n",
      "[Asbru GEM ontologies]\n",
      "0.0934     2  digital electronic guideline library\n",
      "[Digital Electronic Guideline Library, Digital Electronic Guideline Library]\n",
      "0.0824     1  major tool\n",
      "[major tool]\n",
      "0.0755     2  representation\n",
      "[representation, representations]\n",
      "0.0704     1  semantic markup\n",
      "[semantic markup]\n",
      "0.0676     1  clinicians\n",
      "[clinicians]\n",
      "0.0672     1  retrospective assessment\n",
      "[retrospective assessment]\n",
      "0.0670     1  gradual conversion\n",
      "[gradual conversion]\n",
      "0.0652     1  medical care\n",
      "[medical care]\n",
      "0.0647     1  clinical guidelines\n",
      "[Clinical Guidelines]\n",
      "0.0630     1  demonstrated feasibility\n",
      "[demonstrated feasibility]\n",
      "0.0622     1  domain knowledge\n",
      "[domain knowledge]\n",
      "0.0617     1  hybrid meta-ontology\n",
      "[hybrid meta-ontology]\n",
      "0.0582     1  asbru\n",
      "[Asbru]\n",
      "0.0566     1  classification\n",
      "[classification]\n",
      "0.0545     1  run-time application\n",
      "[run-time application]\n",
      "0.0544     1  browsing\n",
      "[browsing]\n",
      "0.0535     1  elements\n",
      "[elements]\n",
      "0.0521     1  context-sensitive search\n",
      "[context-sensitive search]\n",
      "0.0499     1  all ontologies\n",
      "[all ontologies]\n",
      "0.0491     1  point\n",
      "[point]\n",
      "0.0476     1  increasingly sophisticated computational tasks\n",
      "[increasingly sophisticated computational tasks]\n",
      "0.0436     1  web-based, modular, distributed architecture\n",
      "[Web-based, modular, distributed architecture]\n",
      "0.0435     2  degel\n",
      "[DeGeL, DeGeL]\n",
      "0.0365     1  also three guideline-content formats\n",
      "[also three guideline-content formats]\n",
      "0.0364     1  the architecture\n",
      "[The architecture]\n",
      "0.0348     1  all tools\n",
      "[All tools]\n",
      "0.0339     1  these formats\n",
      "[These formats]\n",
      "0.0312     1  the quality\n",
      "[the quality]\n",
      "0.0000     2  we\n",
      "[We, three]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = records[10109]['abstract']\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"hello\"\n",
    "a = b\n",
    "b = \"efg\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
