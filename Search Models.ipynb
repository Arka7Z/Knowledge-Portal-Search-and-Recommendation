{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import fasttext \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import hnswlib\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fasttextModel = fasttext.load_model('crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryEmbedding(query, embeddingType='fastText'):\n",
    "    \n",
    "    '''Get embedding for a single query. Query is pre-processed in this function itself'''\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    query.strip()\n",
    "    query = query.translate(translator)\n",
    "    query = ' '.join(query.split())\n",
    "    \n",
    "    if embeddingType == 'fastText':\n",
    "        embedding = fasttextModel.get_word_vector(query)\n",
    "    elif embeddingType == 'USE':\n",
    "        embedding = embed([query])[0].numpy()\n",
    "        \n",
    "    return np.asarray(embedding)\n",
    "\n",
    "\n",
    "def getQueryEmbeddings(queryList, embeddingType='fastText'):\n",
    "    '''Get embedding list for a list of queries. Query is pre-processed in this function itself''' \n",
    "    embeddings = []\n",
    "    \n",
    "    ## Preprocessing\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    for i in range(len(queryList)):\n",
    "            queryList[i].strip()\n",
    "            queryList[i] = queryList[i].translate(translator)\n",
    "            queryList[i] = ' '.join(queryList[i].split())\n",
    "    \n",
    "    if embeddingType == 'fastText':\n",
    "        for query in queryList:\n",
    "            embedding = fasttextModel.get_word_vector(query)\n",
    "            embeddings.append(embedding)\n",
    "    elif embeddingType == 'USE':\n",
    "        embeddings = embed(queryList).numpy()\n",
    "        \n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentEmbeddings(docIDList, method='abstract', embeddingType='fastText'):\n",
    "    docIDSet = set(docIDList)\n",
    "    embeddingDictForDocs = dict()\n",
    "    \n",
    "    if method=='abstract':\n",
    "        if embeddingType == 'fastText':\n",
    "            filename = './data/dblpAbstract_2Thresholded_FT_Embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_Abstract_2Thresholded_USE_Trans_Embeddings.json'\n",
    "        with open(filename, 'r') as file:\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    if data['id'] in docIDSet:\n",
    "                        embeddingDictForDocs[data['id']] = data['embedding']\n",
    "                        \n",
    "    elif method=='title':\n",
    "        if embeddingType == 'fastText':\n",
    "            filename = './data/dblpTitle_2Thresholded_FT_Embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_Title_2Thresholded_USE_Trans_Embeddings.json'\n",
    "        with open(filename, 'r') as file:\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    if data['id'] in docIDSet:\n",
    "                        embeddingDictForDocs[data['id']] = data['embedding']\n",
    "    \n",
    "    elif method=='fos':\n",
    "        records = []\n",
    "        PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "        i = 0\n",
    "\n",
    "        with open(PapersOutFileName, 'r') as file:\n",
    "            for line in file:\n",
    "                if i % 2 != 0:\n",
    "                    data = json.loads(line)\n",
    "                    if data['id'] in docIDSet:\n",
    "                        records.append(data)\n",
    "                i += 1\n",
    "        \n",
    "        assert len(records) == len(docIDList)\n",
    "        \n",
    "        if embeddingType == 'fastText':\n",
    "            fileName = './data/dblp_fos_FT_Phrase_embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            fileName = './data/dblp_fos_USE_embeddings.json'\n",
    "        \n",
    "        embeddingDict = dict()\n",
    "        with open(fileName, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                embeddingDict[data['fos']] = np.asarray(data['embedding']) \n",
    "        \n",
    "        for record in tqdm(records):\n",
    "            recordEmbeddingList = []\n",
    "            for fos in record['fos']:\n",
    "                recordEmbeddingList.append(embeddingDict[fos])\n",
    "            embeddingDictForDocs[record['id']] = np.mean(recordEmbeddingList, axis = 0)\n",
    "           \n",
    "            \n",
    "    elif method=='fosIdf':\n",
    "        records = []\n",
    "        PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "        i = 0\n",
    "\n",
    "        with open(PapersOutFileName, 'r') as file:\n",
    "            for line in file:\n",
    "                if i % 2 != 0:\n",
    "                    data = json.loads(line)\n",
    "                    records.append(data)\n",
    "                i += 1\n",
    "        \n",
    "        if embeddingType == 'fastText':\n",
    "            fileName = './data/dblp_fos_FT_Phrase_embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_fos_USE_embeddings.json'\n",
    "        \n",
    "        embeddingDict = dict()\n",
    "        fosCount = dict()\n",
    "        N = len(records)\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                embeddingDict[data['fos']] = np.asarray(data['embedding']) \n",
    "                fosCount[data['fos']] = data['count']\n",
    "        \n",
    "        for record in tqdm(records):\n",
    "            recordEmbeddingList = []\n",
    "            weightList = []\n",
    "            for fos in record['fos']:\n",
    "                recordEmbeddingList.append(embeddingDict[fos] * (N / fosCount[fos]))\n",
    "                weightList.append((N / fosCount[fos]))\n",
    "            embeddingDictForDocs[record['id']] = np.mean(recordEmbeddingList, axis = 0) / np.sum(weightList)\n",
    "            \n",
    "    embeddings = []\n",
    "    for docID in docIDList:\n",
    "        embeddings.append(embeddingDictForDocs[docID])\n",
    "    assert len(embeddings) == len(docIDList)\n",
    "    return np.asarray(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def getAllDocumentEmbeddings(method='abstract', embeddingType='fastText'):\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    if method=='abstract':\n",
    "        if embeddingType == 'fastText':\n",
    "            filename = './data/dblpAbstract_2Thresholded_FT_Embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_Abstract_2Thresholded_USE_Trans_Embeddings.json'\n",
    "        with open(filename, 'r') as file:\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    embedding = data['embedding']\n",
    "                    embeddings.append(embedding)\n",
    "    elif method=='title':\n",
    "        if embeddingType == 'fastText':\n",
    "            filename = './data/dblpTitle_2Thresholded_FT_Embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_Title_2Thresholded_USE_Trans_Embeddings.json'\n",
    "        with open(filename, 'r') as file:\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    embedding = data['embedding']\n",
    "                    embeddings.append(embedding)\n",
    "    \n",
    "    elif method=='fos':\n",
    "        records = []\n",
    "        PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "        i = 0\n",
    "\n",
    "        with open(PapersOutFileName, 'r') as file:\n",
    "            for line in file:\n",
    "                if i % 2 != 0:\n",
    "                    data = json.loads(line)\n",
    "                    records.append(data)\n",
    "                i += 1\n",
    "        \n",
    "        if embeddingType == 'fastText':\n",
    "            fileName = './data/dblp_fos_FT_Phrase_embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            filename = './data/dblp_fos_USE_embeddings.json'\n",
    "        \n",
    "        embeddingDict = dict()\n",
    "        with open(fileName, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                embeddingDict[data['fos']] = np.asarray(data['embedding']) \n",
    "        \n",
    "        for record in tqdm(records):\n",
    "            recordEmbeddingList = []\n",
    "            for fos in record['fos']:\n",
    "                recordEmbeddingList.append(embeddingDict[fos])\n",
    "            embeddings.append(np.mean(recordEmbeddingList, axis = 0))\n",
    "            \n",
    "    elif method=='fosIdf':\n",
    "        records = []\n",
    "        PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "        i = 0\n",
    "\n",
    "        with open(PapersOutFileName, 'r') as file:\n",
    "            for line in file:\n",
    "                if i % 2 != 0:\n",
    "                    data = json.loads(line)\n",
    "                    records.append(data)\n",
    "                i += 1\n",
    "        \n",
    "        if embeddingType == 'fastText':\n",
    "            fileName = './data/dblp_fos_FT_Phrase_embeddings.json'\n",
    "        elif embeddingType == 'USE':\n",
    "            fileName = './data/dblp_fos_USE_embeddings.json'\n",
    "        \n",
    "        embeddingDict = dict()\n",
    "        fosCount = dict()\n",
    "        N = len(records)\n",
    "        with open(fileName, 'r') as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                embeddingDict[data['fos']] = np.asarray(data['embedding']) \n",
    "                fosCount[data['fos']] = data['count']\n",
    "        \n",
    "        for record in tqdm(records):\n",
    "            recordEmbeddingList = []\n",
    "            weightList = []\n",
    "            for fos in record['fos']:\n",
    "                recordEmbeddingList.append(embeddingDict[fos] * (N / fosCount[fos]))\n",
    "                weightList.append((N / fosCount[fos]))\n",
    "            embeddings.append(np.mean(recordEmbeddingList, axis = 0) / np.sum(weightList))\n",
    "    \n",
    "    return np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search using Direct embedding Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildIndexer(docEmbeddings):\n",
    "    numElements = len(docEmbeddings)\n",
    "    dimension = len(docEmbeddings[0])\n",
    "    embeddings = np.asarray(docEmbeddings)\n",
    "    data_labels = np.arange(numElements)\n",
    "    p = hnswlib.Index(space = 'cosine', dim = dimension) # possible options are l2, cosine or ip\n",
    "    p.init_index(max_elements = numElements, ef_construction = 200, M = dimension)\n",
    "    p.add_items(embeddings, data_labels)\n",
    "    p.set_ef(30) \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIndexer(filepath, numElements):\n",
    "    p = hnswlib.Index(space='cosine', dim=dimension)  # the space can be changed - keeps the data, alters the distance function.\n",
    "    p.load_index(\"./models/fastTexthnswlib.bin\", max_elements =numElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWithEmbedding(queryList, K=10, method='abstract', embeddingType='fastText'):\n",
    "    IDList = []\n",
    "    with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            IDList.append(data['id'])\n",
    "\n",
    "    docEmbeddings = getAllDocumentEmbeddings(method=method, embeddingType=embeddingType)\n",
    "    queryEmbeddings = getQueryEmbeddings(queryList, embeddingType=embeddingType)\n",
    "    p = buildIndexer(docEmbeddings)\n",
    "\n",
    "    labels, _ = p.knn_query(queryEmbeddings, k = K )\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [IDList[ind] for ind in labels[i]]\n",
    "        \n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "def weight(score, esScore):\n",
    "    embeddingWeight = 0.8\n",
    "    s = embeddingWeight * score + (1 - embeddingWeight) * esScore\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n",
    "\n",
    "def rankList(query, docList, esScoreList=None, K=10):\n",
    "    '''\n",
    "    ReRank documents in the docList wrt the query\n",
    "\n",
    "    Parameters: \n",
    "    query (str): query wrt. which the documents will be ranked\n",
    "\n",
    "    doclist(list[str]): IDs of the documents to rerank, len(doclist >= K)\n",
    "\n",
    "    esScoreList(list[int]): scores of the documents as returned by Elastic Search\n",
    "\n",
    "    k(int): number of top documents to return after re-ranking\n",
    "\n",
    "    Returns: \n",
    "    list[str]: re-ranked list of document IDs\n",
    "    '''\n",
    "    queryEmbedding = getQueryEmbedding(query, embeddingType='fastText')\n",
    "    docEmbeddings = getDocumentEmbeddings(docList, method ='fos', embeddingType='fastText')\n",
    "    docEmbeddings = docEmbeddings.tolist()\n",
    "    cosineSimScores = [ cosine_similarity(queryEmbedding, np.asarray(docEmbedding)) for docEmbedding in docEmbeddings]\n",
    "    if esScoreList is None:\n",
    "        IDsWithScore = [(score, ID) for score, ID in zip(cosineSimScores, docList)]\n",
    "    elif esScoreList is not None:\n",
    "         IDsWithScore = [(weight(score, esScore), ID) for score, ID, esScore in zip(cosineSimScores, docList, esScoreList)]\n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K + 1]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search with Elastic Search\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "\n",
    "def elasticSearch(queryList, index='dblp_v1', K = 10):\n",
    "    fields = ['id', 'title', 'venue', 'authors', 'year', 'abstract', 'fos']\n",
    "    queryBody = {\n",
    "    \"query\": {\n",
    "        \"multi_match\" : {\n",
    "            \"query\" : \"sentence embeddings\",\n",
    "            \"fields\" : ['title', 'abstract', 'authors']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    queryBody['size'] = K\n",
    "    searchResults = []\n",
    "    for query in queryList:\n",
    "        queryBody['query']['multi_match']['query'] = query\n",
    "        res= es.search(index=index,body=queryBody)\n",
    "        searchResults.append([hit['_id'] for hit in res['hits']['hits']])\n",
    "    return searchResults\n",
    "\n",
    "def rankedElasticSearch(queryList, index='dblp_v1', K = 10, includeEsScores=False, rerank=True):\n",
    "    fields = ['id', 'title', 'venue', 'authors', 'year', 'abstract', 'fos']\n",
    "    queryBody = {\n",
    "    \"query\": {\n",
    "        \"multi_match\" : {\n",
    "            \"query\" : \"sentence embeddings\",\n",
    "            \"fields\" : ['title^3', 'abstract', 'authors', 'fos^2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    if rerank:\n",
    "        queryBody['size'] = 2 * K\n",
    "    else:\n",
    "        queryBody['size'] = K\n",
    "    queryBody['query']['multi_match']['fuzziness'] = 'AUTO'\n",
    "    searchResults = []\n",
    "    for query in queryList:\n",
    "        queryBody['query']['multi_match']['query'] = query\n",
    "        res= es.search(index=index,body=queryBody)\n",
    "        initList = [hit['_id'] for hit in res['hits']['hits']]\n",
    "        esScoreList = [hit['_score'] for hit in res['hits']['hits']]\n",
    "        esScoreList = normalize(esScoreList)\n",
    "        if rerank:\n",
    "            if includeEsScores:\n",
    "                searchResults.append(rankList(query, initList, esScoreList = esScoreList))\n",
    "            else:\n",
    "                searchResults.append(rankList(query, initList))\n",
    "        else:\n",
    "            searchResults.append(initList)\n",
    "    return searchResults\n",
    "\n",
    "def rankedMLTElasticSearch(queryList, index='dblp_v1', K = 10, includeEsScores=False):\n",
    "    fields = ['id', 'title', 'venue', 'authors', 'year', 'abstract', 'fos']\n",
    "    queryBody = {\n",
    "    \"query\": {\n",
    "        \"more_like_this\" : {\n",
    "            \"like\" : \"sentence embeddings\",\n",
    "            \"fields\" : ['title^3', 'abstract', 'authors', 'fos^2'],\n",
    "            \"min_term_freq\" : 1\n",
    "        }\n",
    "    }\n",
    "}    \n",
    "    queryBody['size'] = 5 * K\n",
    "    \n",
    "    searchResults = []\n",
    "    for query in queryList:\n",
    "        queryBody['query']['more_like_this']['like'] = query\n",
    "        res= es.search(index=index,body=queryBody)\n",
    "        initList = [hit['_id'] for hit in res['hits']['hits']]\n",
    "        esScoreList = [hit['_score'] for hit in res['hits']['hits']] \n",
    "        esScoreList = normalize(esScoreList)\n",
    "        if includeEsScores:\n",
    "            searchResults.append(rankList(query, initList, esScoreList = esScoreList))\n",
    "        else:\n",
    "            searchResults.append(rankList(query, initList))\n",
    "        \n",
    "        \n",
    "    return searchResults\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['converting word to speech', 'Big data', 'efficient estimation of word representations in vector space', 'natural language interface', 'reinforcement learning in video game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = searchWithEmbedding(queryList, K=10, method='title', embeddingType='USE')\n",
    "results1 = [[str(ID) for ID in sublist] for sublist in results1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = searchWithEmbedding(queryList, K=10, method='fos', embeddingType='fastText')\n",
    "results2 = [[str(ID) for ID in sublist] for sublist in results2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = elasticSearch(queryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea986c91ad648c2ad1e8e1971e581e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results4 = rankedElasticSearch(queryList, index='dblp_v1', K = 10, includeEsScores=False, rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56e86c488c84d0bb4f0d8c44ea2f11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = rankedMLTElasticSearch(queryList, index='dblp_v1', K = 10, includeEsScores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutlIDList = []\n",
    "for r1, r2, r3, r4, r5 in zip(results1, results2, results3, results4, results5):\n",
    "    lis = list(set().union(r1, r2, r3, r4))\n",
    "    resutlIDList.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduce() of empty sequence with no initial value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1cc67061cb11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresutlIDList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reduce() of empty sequence with no initial value"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "IDs = set(reduce(operator.concat, resutlIDList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "i = 0\n",
    "records = dict()\n",
    "with open(PapersOutFileName, 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        if i % 2 != 0:\n",
    "            data = json.loads(line)\n",
    "            if (data['id'] in IDs):\n",
    "                records[data['id']] = {'title': data['title'], 'abstract': data['abstrat'], 'fos': ', '.join(data['fos'])}\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "rows = []\n",
    "    for query, idSubList in tqdm(zip(queryList, resutlIDList)):\n",
    "        for ID in idSubList:\n",
    "            localDict = records[ID]\n",
    "            rows.append([query, ID, localDict['title'], localDict['abstract'], localDict['fos'], 0])\n",
    "random.shuffle(rows)\n",
    "        \n",
    "with open('./data/annotations.csv', \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Query', 'id', 'title', 'abstract', 'fos', 'score']\n",
    "    writer.writerow(header)\n",
    "    for row in rows:\n",
    "            writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
