{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AITopLevelTopics = ['Artificial intelligence', 'Computer vision', 'Data mining',\n",
    "                     'Data science', 'Machine learning', 'Natural language processing',\n",
    "                     'Pattern recognition', 'Speech recognition']\n",
    "with open(\"./data/dblpPaperIDs2Thresholded.json\", 'r') as f:\n",
    "    paperList = json.load(f)\n",
    "papersUnderConsideration = set(paperList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext \n",
    "fasttextModel = fasttext.load_model('crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    with open('./data/dblpAbstract_2Thresholded_FT_Embeddings.json', 'w') as outfile:\n",
    "        for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            abstractWordList = data.get('abstract',[])\n",
    "            abstractString = ' '.join(word for word in abstractWordList)\n",
    "            abstractString = abstractString.replace('\\n', ' ').replace('\\r', '')\n",
    "            embedding = fasttextModel.get_sentence_vector(abstractString).tolist()    # while reading use np.asarray to convert to np array\n",
    "            outDict = dict()\n",
    "            outDict['id'] = paperID\n",
    "            outDict['embedding'] = embedding\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    with open('./data/dblpTitle_2Thresholded_FT_Embeddings.json', 'w') as outfile:\n",
    "        for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            title = data.get('title','')\n",
    "            title = title.replace('\\n', ' ').replace('\\r', '')\n",
    "            embedding = fasttextModel.get_sentence_vector(title).tolist()    # while reading use np.asarray to convert to np array\n",
    "            outDict = dict()\n",
    "            outDict['id'] = paperID\n",
    "            outDict['embedding'] = embedding\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    with open('./data/dblp_Abstract_2Thresholded_USE_Trans_Embeddings.json', 'w') as outfile:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            abstractWordList = data.get('abstract',[])\n",
    "            abstractString = ' '.join(word for word in abstractWordList)\n",
    "            abstractString = abstractString.replace('\\n', ' ').replace('\\r', '')\n",
    "            embedding = embed([abstractString])[0].numpy()\n",
    "            outDict = dict()\n",
    "            outDict['id'] = paperID\n",
    "            outDict['embedding'] = embedding.tolist()\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "paperIDs = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            title = data.get('title','')\n",
    "            title = title.replace('\\n', ' ').replace('\\r', '')\n",
    "            records.append(title)\n",
    "            paperIDs.append(paperID)\n",
    "assert len(records) == len(paperIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100     # block size\n",
    "records = [records[i:i + n] for i in range(0, len(records), n)]\n",
    "paperIDs = [paperIDs[i:i + n] for i in range(0, len(paperIDs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('./data/dblp_Title_2Thresholded_USE_Trans_Embeddings.json', 'w') as outfile:\n",
    "    for i in tqdm(range(len(records))):\n",
    "#         recordSubList = records[i]\n",
    "        paperIDSubList = paperIDs[i]\n",
    "        embeddings = embed(records[i]).numpy().tolist()\n",
    "        for embedding, paperID in zip(embeddings, paperIDSubList):\n",
    "        \n",
    "            outDict = dict()\n",
    "            outDict['id'] = paperID\n",
    "            outDict['embedding'] = embedding#.tolist()\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "paperIDs = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            abstractWordList = data.get('abstract',[])\n",
    "            abstractString = ' '.join(word for word in abstractWordList)\n",
    "            abstractString = abstractString.replace('\\n', ' ').replace('\\r', '')\n",
    "            title = data.get('title','')\n",
    "            title = title.replace('\\n', ' ').replace('\\r', '')\n",
    "            abstractString = title + '. ' + abstractString\n",
    "            records.append(abstractString)\n",
    "            paperIDs.append(paperID)\n",
    "assert len(records) == len(paperIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100     # block size\n",
    "records = [records[i:i + n] for i in range(0, len(records), n)]\n",
    "paperIDs = [paperIDs[i:i + n] for i in range(0, len(paperIDs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('./data/dblp_Abstract_2Thresholded_USE_Trans_Embeddings.json', 'w') as outfile:\n",
    "    for i in tqdm(range(len(records))):\n",
    "#         recordSubList = records[i]\n",
    "        paperIDSubList = paperIDs[i]\n",
    "        embeddings = embed(records[i]).numpy().tolist()\n",
    "        for embedding, paperID in zip(embeddings, paperIDSubList):\n",
    "        \n",
    "            outDict = dict()\n",
    "            outDict['id'] = paperID\n",
    "            outDict['embedding'] = embedding#.tolist()\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476250c26c24895bc8e93ffb2930e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "paperIDs = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperID = data['id'] \n",
    "            abstractWordList = data.get('abstract',[])\n",
    "            abstractString = ' '.join(word for word in abstractWordList)\n",
    "            abstractString = abstractString.replace('\\n', ' ').replace('\\r', '')\n",
    "            title = data.get('title','')\n",
    "            title = title.replace('\\n', ' ').replace('\\r', '')\n",
    "            abstractString = title + '. ' + abstractString\n",
    "            records.append(abstractString)\n",
    "            paperIDs.append(paperID)\n",
    "assert len(records) == len(paperIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47163d4b96664c5b97e0ddbe54cdedcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Stemming the records, punctuations to be removed later by Tf-Idf vectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "stemmedRecords = []\n",
    "for i in tqdm(range(len(records))):\n",
    "    stemmedRecords.append(\" \".join([st.stem(word) for word in records[i].split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac881551c86e4cb3901494fed1cb09d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemRecords = []\n",
    "for i in tqdm(range(len(records))):\n",
    "    lemRecords.append(\" \".join([lemmatizer.lemmatize(word) for word in records[i].split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=2000\n",
    "max_df=1.0\n",
    "min_df=3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8', decode_error='replace', strip_accents='unicode',\n",
    "                    lowercase=True, analyzer='word', stop_words='english',\n",
    "                    token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_-]+\\b',\n",
    "                    ngram_range=(1, 1), max_features=max_features,\n",
    "                    norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                    max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = v.fit_transform(lemRecords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dblpAbstract_2Thresholded_TfIdfUni_Embeddings.json', 'w') as outfile:\n",
    "    for i in range(len(paperIDs)):\n",
    "        outDict = dict()\n",
    "        paperID = paperIDs[i]\n",
    "        embedding =  X[i].tolist()[0]\n",
    "        outDict['id'] = paperID\n",
    "        outDict['embedding'] = embedding\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8', decode_error='replace', strip_accents='unicode',\n",
    "                    lowercase=True, analyzer='word', stop_words='english',\n",
    "                    token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_-]+\\b',\n",
    "                    ngram_range=(1, 2), max_features=max_features,\n",
    "                    norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                    max_df=max_df, min_df=min_df)\n",
    "X = v.fit_transform(lemRecords)\n",
    "X = X.astype(np.float32).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc104303bfd46d3896d710f8061b2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=475839.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/dblpAbstract_2Thresholded_TfIdfUni_Embeddings.json', 'w') as outfile:\n",
    "    for i in tqdm(range(len(paperIDs))):\n",
    "        outDict = dict()\n",
    "        paperID = paperIDs[i]\n",
    "        embedding =  X[i].tolist()[0]\n",
    "        outDict['id'] = paperID\n",
    "        outDict['embedding'] = embedding\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = X[0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'absolute', 'abstract', 'abstract paper', 'abstraction', 'access', 'according', 'account', 'accuracy', 'accurate', 'accurately', 'achieve', 'achieved', 'achieves', 'achieving', 'acoustic', 'acquired', 'acquisition', 'act', 'action', 'actions', 'activation', 'active', 'activities', 'activity', 'actual', 'adapt', 'adaptation', 'adapted', 'adaptive', 'adaptively', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'address', 'address problem', 'addressed', 'adopt', 'adopted', 'advance', 'advanced', 'advantage', 'affect', 'affected', 'age', 'agent', 'agents', 'aggregate', 'aggregation', 'agreement', 'ai', 'aid', 'aim', 'aimed', 'al', 'algorithm', 'algorithm based', 'algorithm proposed', 'algorithmic', 'algorithms', 'alignment', 'allocation', 'allow', 'allowing', 'allows', 'alternative', 'ambiguity', 'analysis', 'analytical', 'analytics', 'analyze', 'analyzed', 'analyzing', 'angle', 'animation', 'annotated', 'annotation', 'anomaly', 'answer', 'answering', 'appear', 'appearance', 'applicability', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'approach', 'approach based', 'approaches', 'appropriate', 'approximate', 'approximately', 'approximation', 'arabic', 'arbitrary', 'architecture', 'area', 'areas', 'argue', 'array', 'art', 'article', 'artifact', 'artificial', 'artificial intelligence', 'artificial neural', 'aspect', 'ass', 'assessing', 'assessment', 'assigned', 'assignment', 'assist', 'associated', 'association', 'assume', 'assumed', 'assumption', 'attack', 'attempt', 'attention', 'attribute', 'attributes', 'audio', 'augmented', 'author', 'automated', 'automatic', 'automatically', 'autonomous', 'availability', 'available', 'average', 'avoid', 'background', 'balance', 'base', 'based', 'baseline', 'basic', 'basis', 'bayes', 'bayesian', 'bayesian network', 'behavior', 'behaviour', 'belief', 'benchmark', 'benefit', 'best', 'better', 'better performance', 'bias', 'big', 'binary', 'biological', 'biomedical', 'biometric', 'bit', 'blind', 'block', 'body', 'boosting', 'bound', 'boundary', 'brain', 'broad', 'build', 'building', 'built', 'business', 'calculate', 'calculated', 'calculation', 'calibration', 'called', 'camera', 'cameras', 'cancer', 'candidate', 'capability', 'capable', 'capacity', 'capture', 'captured', 'capturing', 'care', 'carried', 'carry', 'case', 'case study', 'cases', 'categories', 'categorization', 'category', 'cause', 'caused', 'cell', 'center', 'central', 'certain', 'chain', 'challenge', 'challenges', 'challenging', 'change', 'changes', 'changing', 'channel', 'character', 'characteristic', 'characteristics', 'characterization', 'characterize', 'characterized', 'chinese', 'choice', 'choose', 'chosen', 'citation', 'class', 'classes', 'classical', 'classification', 'classification accuracy', 'classified', 'classifier', 'classifiers', 'classify', 'classifying', 'clear', 'clearly', 'clinical', 'close', 'closed', 'closely', 'cloud', 'cluster', 'clustering', 'clustering algorithm', 'clusters', 'cnn', 'code', 'coding', 'coefficient', 'coefficients', 'cognitive', 'coherence', 'coherent', 'collaborative', 'collected', 'collection', 'color', 'combination', 'combinatorial', 'combine', 'combined', 'combining', 'come', 'commercial', 'common', 'commonly', 'commonly used', 'communication', 'community', 'compact', 'comparable', 'comparative', 'compare', 'compared', 'comparing', 'comparison', 'compensation', 'competition', 'competitive', 'complementary', 'complete', 'completely', 'complex', 'complexity', 'complicated', 'component', 'component analysis', 'components', 'composed', 'composition', 'comprehensive', 'compressed', 'compression', 'computation', 'computational', 'computational complexity', 'computational cost', 'computationally', 'compute', 'computed', 'computer', 'computer vision', 'computing', 'concept', 'concepts', 'conceptual', 'concern', 'concerning', 'conclude', 'conclusion', 'condition', 'conditional', 'conditions', 'conduct', 'conducted', 'confidence', 'configuration', 'confirm', 'connected', 'connection', 'connectivity', 'consequence', 'consequently', 'consider', 'considerable', 'consideration', 'considered', 'considering', 'considers', 'consistency', 'consistent', 'consistently', 'consisting', 'consists', 'constant', 'constrained', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'contact', 'contain', 'containing', 'contains', 'content', 'content-based', 'context', 'contextual', 'continuous', 'contour', 'contrast', 'contribute', 'contribution', 'control', 'controlled', 'controller', 'conventional', 'convergence', 'convex', 'convolutional', 'convolutional neural', 'cooperative', 'coordinate', 'cope', 'core', 'corpora', 'corpus', 'correct', 'correction', 'correctly', 'correlated', 'correlation', 'correspondence', 'corresponding', 'cost', 'coupled', 'course', 'covariance', 'cover', 'coverage', 'create', 'created', 'creating', 'creation', 'criteria', 'criterion', 'critical', 'cross', 'crucial', 'ct', 'cue', 'current', 'currently', 'curve', 'customer', 'data', 'data mining', 'data paper', 'data set', 'data sets', 'data-driven', 'database', 'databases', 'dataset', 'datasets', 'date', 'deal', 'dealing', 'decade', 'decision', 'decision making', 'decision support', 'decision tree', 'decision-making', 'decoding', 'decomposition', 'decrease', 'deep', 'deep learning', 'define', 'defined', 'defining', 'definition', 'deformable', 'deformation', 'degree', 'demand', 'demonstrate', 'demonstrate effectiveness', 'demonstrated', 'demonstrates', 'denoising', 'dense', 'density', 'depend', 'dependency', 'dependent', 'depending', 'depends', 'depth', 'derive', 'derived', 'described', 'describes', 'describing', 'description', 'descriptor', 'descriptors', 'design', 'designed', 'designing', 'desirable', 'desired', 'despite', 'detailed', 'detect', 'detected', 'detecting', 'detection', 'detector', 'determination', 'determine', 'determined', 'determining', 'deterministic', 'develop', 'developed', 'developing', 'development', 'device', 'devices', 'diagnosis', 'diagnostic', 'dialogue', 'dictionary', 'difference', 'different', 'different type', 'differential', 'difficult', 'difficulty', 'diffusion', 'digital', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'direct', 'direction', 'directly', 'disambiguation', 'discover', 'discovered', 'discovering', 'discovery', 'discrete', 'discriminant', 'discriminant analysis', 'discrimination', 'discriminative', 'discus', 'discussed', 'discussion', 'disease', 'display', 'distance', 'distinct', 'distinguish', 'distortion', 'distributed', 'distribution', 'distributions', 'diverse', 'diversity', 'divided', 'document', 'documents', 'doe', 'doe require', 'domain', 'domains', 'drawback', 'driven', 'duration', 'dynamic', 'dynamically', 'dynamics', 'earlier', 'early', 'easily', 'easy', 'edge', 'edges', 'eeg', 'effect', 'effective', 'effectively', 'effectiveness', 'effectiveness proposed', 'effects', 'efficacy', 'efficiency', 'efficient', 'efficient algorithm', 'efficiently', 'effort', 'electronic', 'element', 'em', 'embedded', 'embedding', 'emerging', 'emotion', 'empirical', 'empirically', 'employ', 'employed', 'employing', 'enable', 'enables', 'enabling', 'encoded', 'encoding', 'end', 'energy', 'engine', 'engineering', 'english', 'enhance', 'enhanced', 'enhancement', 'enhancing', 'ensemble', 'ensure', 'entire', 'entity', 'entropy', 'environment', 'environmental', 'environments', 'equal', 'equation', 'equivalent', 'error', 'error rate', 'errors', 'especially', 'essential', 'establish', 'established', 'estimate', 'estimated', 'estimating', 'estimation', 'estimator', 'et', 'et al', 'evaluate', 'evaluated', 'evaluating', 'evaluation', 'event', 'events', 'evidence', 'evolution', 'evolutionary', 'evolutionary algorithm', 'evolving', 'exact', 'examine', 'examined', 'example', 'examples', 'excellent', 'exchange', 'execution', 'exhibit', 'exist', 'existence', 'existing', 'existing method', 'exists', 'expansion', 'expected', 'expensive', 'experience', 'experiment', 'experimental', 'experimental evaluation', 'experimental result', 'experimentally', 'experiments', 'expert', 'experts', 'explain', 'explanation', 'explicit', 'explicitly', 'exploit', 'exploited', 'exploiting', 'exploration', 'explore', 'explored', 'explores', 'exploring', 'express', 'expressed', 'expression', 'expressions', 'expressive', 'extend', 'extended', 'extending', 'extends', 'extension', 'extensive', 'extensive experiment', 'extensively', 'extent', 'external', 'extract', 'extracted', 'extracting', 'extraction', 'extreme', 'extremely', 'eye', 'face', 'face recognition', 'facial', 'facilitate', 'fact', 'factor', 'factorization', 'factors', 'fail', 'failure', 'false', 'family', 'far', 'fashion', 'fast', 'faster', 'fault', 'feasibility', 'feasible', 'feature', 'feature extraction', 'feature selection', 'feature space', 'features', 'feedback', 'field', 'fields', 'file', 'filter', 'filtering', 'filters', 'final', 'finally', 'finding', 'fingerprint', 'finite', 'firstly', 'fit', 'fitness', 'fitting', 'fixed', 'flexibility', 'flexible', 'flow', 'focus', 'focused', 'focusing', 'followed', 'following', 'force', 'forecasting', 'foreground', 'forest', 'form', 'formal', 'formalism', 'format', 'formation', 'formulate', 'formulated', 'formulation', 'forward', 'foundation', 'frame', 'frames', 'framework', 'free', 'frequency', 'frequent', 'frequently', 'fully', 'function', 'functional', 'functionality', 'functions', 'fundamental', 'furthermore', 'fusion', 'future', 'fuzzy', 'ga', 'gain', 'game', 'gap', 'gaussian', 'gaussian mixture', 'gene', 'general', 'generalization', 'generalized', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generative', 'generic', 'genetic', 'genetic algorithm', 'genetic programming', 'geometric', 'geometry', 'gesture', 'given', 'global', 'goal', 'good', 'gradient', 'grammar', 'graph', 'graphic', 'graphical', 'graphs', 'great', 'greater', 'greatly', 'grid', 'ground', 'ground truth', 'group', 'grouping', 'groups', 'growing', 'growth', 'guarantee', 'guide', 'guided', 'ha', 'ha shown', 'hand', 'handle', 'handling', 'handwritten', 'hard', 'hardware', 'having', 'head', 'health', 'heart', 'help', 'heterogeneous', 'heuristic', 'hidden', 'hidden markov', 'hierarchical', 'hierarchy', 'high', 'high accuracy', 'high-dimensional', 'high-level', 'high-quality', 'high-resolution', 'higher', 'highest', 'highlight', 'highly', 'histogram', 'historical', 'history', 'hmm', 'hold', 'http', 'huge', 'human', 'hybrid', 'hyperspectral', 'hypothesis', 'idea', 'identification', 'identified', 'identifies', 'identify', 'identifying', 'identity', 'ii', 'iii', 'illumination', 'illustrate', 'illustrated', 'image', 'image processing', 'image retrieval', 'image segmentation', 'image using', 'imagery', 'images', 'imaging', 'impact', 'implement', 'implementation', 'implemented', 'implementing', 'implication', 'implicit', 'importance', 'important', 'important role', 'improve', 'improve performance', 'improved', 'improvement', 'improves', 'improving', 'include', 'included', 'includes', 'including', 'incomplete', 'incorporate', 'incorporated', 'incorporates', 'incorporating', 'increase', 'increased', 'increasing', 'increasingly', 'incremental', 'independent', 'independently', 'index', 'indexing', 'indicate', 'indicates', 'indicator', 'individual', 'indoor', 'induced', 'induction', 'industrial', 'industry', 'infer', 'inference', 'influence', 'information', 'information retrieval', 'informative', 'infrastructure', 'inherent', 'initial', 'input', 'inside', 'insight', 'inspired', 'instance', 'instances', 'instead', 'integrate', 'integrated', 'integrates', 'integrating', 'integration', 'intelligence', 'intelligent', 'intended', 'intensity', 'interact', 'interaction', 'interactions', 'interactive', 'interesting', 'interface', 'intermediate', 'internal', 'international', 'internet', 'interpolation', 'interpretation', 'interval', 'intrinsic', 'introduce', 'introduce new', 'introduced', 'introduces', 'introducing', 'introduction', 'intuitive', 'invariant', 'inverse', 'investigate', 'investigated', 'investigates', 'investigation', 'involve', 'involved', 'involves', 'involving', 'ir', 'issue', 'issues', 'item', 'iteration', 'iterative', 'iteratively', 'joint', 'jointly', 'journal', 'just', 'k-means', 'kernel', 'key', 'keywords', 'kind', 'knowledge', 'knowledge base', 'known', 'label', 'labeled', 'labeling', 'labels', 'lack', 'landmark', 'language', 'language processing', 'languages', 'large', 'large number', 'large scale', 'large-scale', 'largely', 'larger', 'laser', 'latent', 'layer', 'lda', 'le', 'lead', 'leading', 'learn', 'learned', 'learner', 'learning', 'learning algorithm', 'learning method', 'learns', 'left', 'length', 'letter', 'level', 'levels', 'leverage', 'lexical', 'lexicon', 'library', 'lie', 'life', 'light', 'lighting', 'like', 'likelihood', 'likely', 'limit', 'limitation', 'limited', 'line', 'linear', 'linguistic', 'link', 'linked', 'list', 'literature', 'little', 'local', 'localization', 'locally', 'location', 'log', 'logic', 'logical', 'long', 'look', 'loss', 'lot', 'low', 'low-level', 'lower', 'machine', 'machine learning', 'machine svm', 'machine translation', 'machines', 'magnetic', 'magnetic resonance', 'magnitude', 'main', 'mainly', 'maintain', 'maintaining', 'maintenance', 'major', 'majority', 'make', 'make use', 'making', 'management', 'manifold', 'manipulation', 'manner', 'manual', 'manually', 'map', 'mapping', 'maps', 'margin', 'market', 'markov', 'markov model', 'match', 'matched', 'matching', 'material', 'mathematical', 'matrix', 'maximum', 'mean', 'meaning', 'meaningful', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'measuring', 'mechanism', 'medical', 'medium', 'meet', 'membership', 'memory', 'merging', 'mesh', 'message', 'metadata', 'method', 'method based', 'method proposed', 'method used', 'method using', 'methodology', 'methods', 'metric', 'metrics', 'million', 'minimal', 'minimization', 'minimize', 'minimizing', 'minimum', 'mining', 'missing', 'mixed', 'mixture', 'mobile', 'mobile robot', 'modality', 'mode', 'model', 'model based', 'model-based', 'modeled', 'modeling', 'modelling', 'models', 'modern', 'modification', 'modified', 'module', 'monitoring', 'morphological', 'motion', 'motivated', 'motivation', 'movement', 'moving', 'mr', 'mri', 'multi-objective', 'multidimensional', 'multimedia', 'multimodal', 'multiple', 'multivariate', 'music', 'mutual', 'naive', 'named', 'natural', 'natural language', 'naturally', 'nature', 'navigation', 'near', 'nearest', 'nearest neighbor', 'necessary', 'need', 'needed', 'negative', 'neighbor', 'neighborhood', 'net', 'network', 'networks', 'neural', 'neural network', 'neural networks', 'neuron', 'new', 'new algorithm', 'new approach', 'new method', 'news', 'node', 'nodes', 'noise', 'noisy', 'non-linear', 'nonlinear', 'normal', 'normalization', 'normalized', 'notion', 'novel', 'novel approach', 'novel method', 'number', 'numerical', 'numerous', 'object', 'object recognition', 'objective', 'objective function', 'objects', 'observation', 'observations', 'observe', 'observed', 'obstacle', 'obtain', 'obtained', 'obtaining', 'occlusion', 'occur', 'occurrence', 'offer', 'on-line', 'ones', 'online', 'ontologies', 'ontology', 'open', 'operating', 'operation', 'operations', 'operator', 'operators', 'opinion', 'opportunity', 'optical', 'optimal', 'optimization', 'optimization problem', 'optimize', 'optimized', 'optimizing', 'optimum', 'order', 'order magnitude', 'ordering', 'organization', 'orientation', 'oriented', 'original', 'outcome', 'outlier', 'outline', 'outperform', 'outperforms', 'output', 'overall', 'overcome', 'overlap', 'overlapping', 'overview', 'page', 'pair', 'pairwise', 'paper', 'paper address', 'paper describes', 'paper discus', 'paper focus', 'paper introduce', 'paper introduces', 'paper investigate', 'paper new', 'paper present', 'paper propose', 'paper proposes', 'paper study', 'paradigm', 'parallel', 'parameter', 'parameters', 'parametric', 'parser', 'parsing', 'partial', 'partially', 'participant', 'particle', 'particle swarm', 'particular', 'particularly', 'partition', 'partitioning', 'parts', 'past', 'patch', 'path', 'patient', 'pattern', 'pattern recognition', 'patterns', 'pca', 'people', 'perceived', 'perception', 'perceptual', 'perform', 'performance', 'performance proposed', 'performed', 'performing', 'performs', 'period', 'person', 'personal', 'personalized', 'perspective', 'phase', 'phenomenon', 'phone', 'phrase', 'physical', 'picture', 'pipeline', 'pixel', 'pixels', 'place', 'plan', 'planar', 'plane', 'planning', 'platform', 'play', 'play important', 'point', 'points', 'policy', 'polynomial', 'poor', 'popular', 'population', 'pose', 'position', 'positive', 'possibility', 'possible', 'possibly', 'posterior', 'potential', 'potentially', 'power', 'powerful', 'practical', 'practice', 'precise', 'precisely', 'precision', 'predict', 'predicted', 'predicting', 'prediction', 'predictive', 'predictor', 'preference', 'preliminary', 'preprocessing', 'presence', 'present', 'present approach', 'present method', 'present new', 'present novel', 'presentation', 'presented', 'preserve', 'preserving', 'previous', 'previous work', 'previously', 'primary', 'primitive', 'principal', 'principal component', 'principle', 'prior', 'prior knowledge', 'priori', 'privacy', 'probabilistic', 'probability', 'problem', 'problems', 'procedure', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'producing', 'product', 'production', 'profile', 'program', 'programming', 'progress', 'project', 'projection', 'promising', 'propagation', 'proper', 'properties', 'property', 'proposal', 'propose', 'propose method', 'propose new', 'propose novel', 'proposed', 'proposed algorithm', 'proposed approach', 'proposed method', 'proposes', 'protein', 'protocol', 'prototype', 'prove', 'proved', 'proven', 'provide', 'provided', 'provides', 'providing', 'pruning', 'pso', 'public', 'publication', 'publicly', 'publicly available', 'published', 'purpose', 'purposes', 'quadratic', 'qualitative', 'quality', 'quantitative', 'quantization', 'queries', 'query', 'querying', 'question', 'quickly', 'quite', 'radar', 'radial', 'random', 'random field', 'randomly', 'range', 'rank', 'ranking', 'rapid', 'rapidly', 'rate', 'rates', 'rating', 'ratio', 'raw', 'reach', 'reading', 'real', 'real data', 'real time', 'real world', 'real-life', 'real-time', 'real-world', 'realistic', 'reality', 'reason', 'reasonable', 'reasoning', 'recall', 'received', 'recent', 'recent years', 'recently', 'recognition', 'recognition paper', 'recognition using', 'recognize', 'recognized', 'recognizing', 'recommendation', 'recommender', 'reconstructed', 'reconstruction', 'record', 'recorded', 'recording', 'recover', 'recovery', 'recurrent', 'recursive', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'redundancy', 'redundant', 'reference', 'refinement', 'reflect', 'regard', 'regarding', 'region', 'regions', 'registration', 'regression', 'regular', 'regularization', 'reinforcement', 'reinforcement learning', 'related', 'relation', 'relational', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relevance', 'relevant', 'reliability', 'reliable', 'relies', 'rely', 'remains', 'remote', 'removal', 'remove', 'rendering', 'report', 'reported', 'repository', 'represent', 'representation', 'representations', 'representative', 'represented', 'representing', 'represents', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'research', 'researcher', 'resolution', 'resonance', 'resource', 'resources', 'respect', 'respectively', 'response', 'restricted', 'result', 'result demonstrate', 'result indicate', 'result obtained', 'result proposed', 'resulting', 'results', 'retrieval', 'retrieve', 'return', 'reveal', 'review', 'rich', 'right', 'risk', 'road', 'robot', 'robotic', 'robotics', 'robots', 'robust', 'robustness', 'role', 'rotation', 'rough', 'rule', 'rule-based', 'rules', 'run', 'running', 'saliency', 'salient', 'sample', 'samples', 'sampling', 'sar', 'satisfactory', 'scalability', 'scalable', 'scale', 'scaling', 'scan', 'scenario', 'scenarios', 'scene', 'scenes', 'schema', 'scheme', 'schemes', 'science', 'scientific', 'score', 'scoring', 'search', 'searching', 'second', 'security', 'seen', 'segment', 'segmentation', 'segmented', 'select', 'selected', 'selecting', 'selection', 'self-organizing', 'semantic', 'semantic web', 'semantically', 'semantics', 'semi-supervised', 'sense', 'sensing', 'sensitive', 'sensitivity', 'sensor', 'sensors', 'sentence', 'sentiment', 'separate', 'separately', 'separation', 'sequence', 'sequences', 'sequential', 'series', 'serve', 'service', 'services', 'set', 'sets', 'setting', 'settings', 'shape', 'share', 'shared', 'sharing', 'shift', 'short', 'showed', 'showing', 'shown', 'sign', 'signal', 'signals', 'signature', 'significance', 'significant', 'significant improvement', 'significantly', 'similar', 'similarity', 'similarity measure', 'simple', 'simply', 'simulated', 'simulation', 'simulation result', 'simulations', 'simultaneous', 'simultaneously', 'single', 'site', 'situation', 'situations', 'size', 'small', 'smaller', 'smart', 'smooth', 'smoothing', 'so-called', 'social', 'social network', 'soft', 'software', 'solution', 'solutions', 'solve', 'solve problem', 'solved', 'solving', 'sophisticated', 'sound', 'source', 'sources', 'space', 'spaces', 'sparse', 'sparsity', 'spatial', 'spatially', 'spatio-temporal', 'speaker', 'special', 'specific', 'specifically', 'specification', 'specified', 'spectral', 'spectrum', 'speech', 'speech recognition', 'speed', 'spoken', 'square', 'stability', 'stable', 'stage', 'standard', 'start', 'starting', 'state', 'state art', 'state-of-the-art', 'states', 'static', 'statistic', 'statistical', 'statistically', 'statistics', 'step', 'steps', 'stereo', 'stochastic', 'storage', 'store', 'stored', 'strategies', 'strategy', 'stream', 'streams', 'strength', 'string', 'strong', 'strongly', 'structural', 'structure', 'structured', 'structures', 'student', 'studied', 'studies', 'study', 'style', 'subject', 'subjective', 'subjects', 'subsequent', 'subsequently', 'subset', 'subspace', 'substantial', 'substantially', 'success', 'successful', 'successfully', 'suffer', 'sufficient', 'suggest', 'suggested', 'suggests', 'suitable', 'suited', 'sum', 'summarization', 'summary', 'superior', 'superiority', 'supervised', 'support', 'support vector', 'supported', 'supporting', 'surface', 'surfaces', 'surveillance', 'survey', 'svm', 'svms', 'swarm', 'swarm optimization', 'symbolic', 'syntactic', 'synthesis', 'synthetic', 'synthetic real', 'systematic', 'systems', 'table', 'tackle', 'tag', 'tagging', 'taken', 'taking', 'target', 'task', 'tasks', 'taxonomy', 'team', 'technical', 'technique', 'techniques', 'technology', 'template', 'temporal', 'tend', 'tensor', 'term', 'terms', 'test', 'tested', 'testing', 'text', 'textual', 'texture', 'theoretical', 'theory', 'thousand', 'three-dimensional', 'threshold', 'thresholding', 'time', 'time series', 'times', 'tissue', 'today', 'tool', 'tools', 'topic', 'topological', 'topology', 'total', 'track', 'tracker', 'tracking', 'trade-off', 'traditional', 'traffic', 'train', 'trained', 'training', 'training data', 'training set', 'trajectory', 'transfer', 'transform', 'transformation', 'transformed', 'transforms', 'transition', 'translation', 'transmission', 'treatment', 'tree', 'trees', 'trend', 'true', 'truth', 'try', 'tuning', 'turn', 'twitter', 'two-dimensional', 'type', 'types', 'typical', 'typically', 'uncertain', 'uncertainty', 'underlying', 'understand', 'understanding', 'unfortunately', 'unified', 'uniform', 'unique', 'unit', 'university', 'unknown', 'unlabeled', 'unlike', 'unsupervised', 'update', 'updated', 'updating', 'upper', 'urban', 'usage', 'use', 'used', 'useful', 'usefulness', 'user', 'users', 'using', 'usually', 'utility', 'utilize', 'utilized', 'utilizes', 'utilizing', 'utterance', 'validate', 'validated', 'validation', 'validity', 'valuable', 'value', 'values', 'variability', 'variable', 'variables', 'variance', 'variant', 'variation', 'variational', 'variety', 'various', 'varying', 'vector', 'vector machine', 'vector machines', 'vectors', 'vehicle', 'velocity', 'verification', 'verified', 'verify', 'version', 'video', 'videos', 'view', 'viewed', 'viewing', 'viewpoint', 'views', 'virtual', 'visible', 'vision', 'visual', 'visualization', 'visually', 'vocabulary', 'voice', 'volume', 'voting', 'wa', 'watermarking', 'wavelet', 'wavelet transform', 'way', 'weak', 'web', 'weight', 'weighted', 'weighting', 'well-known', 'wide', 'wide range', 'widely', 'widely used', 'window', 'word', 'words', 'work', 'workflow', 'working', 'world', 'written', 'xml', 'year', 'years', 'yield']\n"
     ]
    }
   ],
   "source": [
    " print(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'region'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d37f3f4a4064edcb2d51ce0cb01072e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paperIDs = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "            data = json.loads(line)\n",
    "            paperIDs.append(data['id'] )\n",
    "\n",
    "with open(\"./data/orderedPaperIDs.json\", 'w') as f:\n",
    "    json.dump(paperIDs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
