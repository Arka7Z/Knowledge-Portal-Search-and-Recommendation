{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getSpotsAndEntities(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'false', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "def embed(inputTexts):\n",
    "    return model.encode(inputTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity -Similarity Matrix Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def l1similarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return 1 / ( 1+ np.linalg.norm((a - b), ord=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictForTitles = dict()\n",
    "dictForBody = dict()\n",
    "titleDictLoaded = False\n",
    "bodyDictLoaded = False\n",
    "queryEntitySpotDict = dict()\n",
    "def getQueryEntitiesAndSpots(query, long_text = 0):\n",
    "    global queryEntitySpotDict \n",
    "    if query in queryEntitySpotDict:\n",
    "        return queryEntitySpotDict[query]['entities'], queryEntitySpotDict[query]['spots']\n",
    "    else:\n",
    "        spotsWithFreq, entitiesWithFreq = getSpotsAndEntities(query, long_text = 0)\n",
    "        queryEntitySpotDict[query]= {'entities': entitiesWithFreq,'spots' : spotsWithFreq}\n",
    "        return entitiesWithFreq, spotsWithFreq\n",
    "def loadEntityDict(method='title'):\n",
    "    global dictForTitles\n",
    "    global dictForBody\n",
    "    if method == 'title':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForTitles = json.loads(line)\n",
    "    elif method == 'body':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForBody = json.loads(line)\n",
    "def retrieveSpots(docID, method='title'):\n",
    "    '''Returns pre computed spot mentions for this docID, where each element is a tuple of (spot name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['spots']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['spots']\n",
    "    \n",
    "def retrieveEntities(docID, method='title'):\n",
    "    '''Returns pre computed entities for this docID, where each element is a tuple of (entity name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['entities']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['entities']\n",
    "    \n",
    "def retrieveEntityEmbedding(entity):\n",
    "    try:\n",
    "        return entityEmbeddingDict[entity]\n",
    "    except:\n",
    "        return  embed([entity])[0]\n",
    "    \n",
    "\n",
    "def computeSimilarityMatrix(query, text, method = 'title'):\n",
    "    _, queryEntitiesWithFreq = getQueryEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    _, docEntitiesWithFreq = getSpotsAndEntities(text)\n",
    "    docEntityFrequencies = [entityTuple[1] for entityTuple in docEntitiesWithFreq]\n",
    "    queryEntityFrequencies = [entityTuple[1] for entityTuple in queryEntitiesWithFreq]\n",
    "\n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityEmbeddings = embed(queryEntities)\n",
    "    docEntityEmbeddings = embed([preprocess(entityTuple[0]) for entityTuple in docEntitiesWithFreq])\n",
    "\n",
    "    \n",
    "    numDocEntities = len(docEntitiesWithFreq)\n",
    "    numQueryEntities = len(queryEntitiesWithFreq)\n",
    "    simMatrix = np.zeros((numDocEntities, numQueryEntities))\n",
    "    for i in range(numDocEntities):\n",
    "        for j in range(numQueryEntities):\n",
    "            simMatrix[i][j] = max(0, cosineSimilarity(docEntityEmbeddings[i], queryEntityEmbeddings[j]))\n",
    "    return simMatrix,  queryEntityFrequencies, docEntityFrequencies\n",
    "\n",
    "def reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = 'column', pooling = 'max'):\n",
    "    if axis == 'column':\n",
    "        axis = 0\n",
    "    else:\n",
    "        axis = 1\n",
    "    if pooling == 'max':\n",
    "        try:\n",
    "            return np.max(simMatrix, axis = axis) # along columns\n",
    "        except:\n",
    "            return np.zeros(1)\n",
    "    \n",
    "def reduceVector(vector, reduction = 'avg'):\n",
    "    if not  vector.size:\n",
    "        return 0\n",
    "    if reduction == 'avg':\n",
    "        return sum(vector) / len(vector)\n",
    "    \n",
    "def semanticScore(query, text, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeSimilarityMatrix(query, text, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5283d998a4097a31fa6f404a26377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'][0])\n",
    "        IDList.append(data['docno'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a07884ad37484a854380893056bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = dict()\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        records[data['docno']] = (data['title'][0], data['numCitedBy'][0], data['numKeyCitations'][0])\n",
    "        \n",
    "def getPaperDetails(docno):\n",
    "    return records[docno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e309d672d643d2a0ba77289ed2c1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queryList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_query.json') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        queryList.append(data['query'])\n",
    "queryEntityList = []\n",
    "for query in tqdm(queryList):\n",
    "    _, queryEntitiesWithFreq = getQueryEntitiesAndSpots(query, long_text = 0) \n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityList.append(queryEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec64ceb3f2347f5b93b2a8cdaccab20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titleList = []\n",
    "citationList =  []\n",
    "keyCitationList = []\n",
    "queryIDList = []\n",
    "Y = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2.qrel') as file:\n",
    "    for line in tqdm(file):\n",
    "        lineString = line.split()\n",
    "        qid = int(lineString[0]) - 1    # - 1 to account for our 0 based indexing, while theirs is 1 based\n",
    "        docno = lineString[2]\n",
    "        relScore = int(lineString[-1])\n",
    "        if docno in records:\n",
    "            title, citations, keyCitations = records[docno]\n",
    "            queryIDList.append(qid)\n",
    "            titleList.append(title)\n",
    "            citationList.append(citations)\n",
    "            keyCitationList.append(keyCitations)\n",
    "            Y.append(relScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56bf512579d4b3b811b0dc71896b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "semScores = []\n",
    "for qid, title in tqdm(zip(queryIDList, titleList)):\n",
    "    semScores.append(semanticScore(queryList[qid], title, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## semScores citationList and keyCitationList are the feature vectors, Y to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for score, citation, keyCitation in zip(semScores, citationList, keyCitationList):\n",
    "    X.append([score, citation, keyCitation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Explicit_Semantic_Ranking_Dataset/trainData.json', 'w') as outfile:\n",
    "    tmp = dict()\n",
    "    tmp['X'] = X\n",
    "    tmp['Y'] = Y\n",
    "    json.dump(tmp, outfile)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
