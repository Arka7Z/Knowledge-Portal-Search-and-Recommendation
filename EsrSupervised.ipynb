{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getSpotsAndEntities(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'false', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "def embed(inputTexts):\n",
    "    return model.encode(inputTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity -Similarity Matrix Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def l1similarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return 1 / ( 1+ np.linalg.norm((a - b), ord=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictForTitles = dict()\n",
    "dictForBody = dict()\n",
    "titleDictLoaded = False\n",
    "bodyDictLoaded = False\n",
    "queryEntitySpotDict = dict()\n",
    "def getQueryEntitiesAndSpots(query, long_text = 0):\n",
    "    global queryEntitySpotDict \n",
    "    if query in queryEntitySpotDict:\n",
    "        return queryEntitySpotDict[query]['entities'], queryEntitySpotDict[query]['spots']\n",
    "    else:\n",
    "        spotsWithFreq, entitiesWithFreq = getSpotsAndEntities(query, long_text = 0)\n",
    "        queryEntitySpotDict[query]= {'entities': entitiesWithFreq,'spots' : spotsWithFreq}\n",
    "        return entitiesWithFreq, spotsWithFreq\n",
    "def loadEntityDict(method='title'):\n",
    "    global dictForTitles\n",
    "    global dictForBody\n",
    "    if method == 'title':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForTitles = json.loads(line)\n",
    "    elif method == 'body':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForBody = json.loads(line)\n",
    "def retrieveSpots(docID, method='title'):\n",
    "    '''Returns pre computed spot mentions for this docID, where each element is a tuple of (spot name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['spots']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['spots']\n",
    "    \n",
    "def retrieveEntities(docID, method='title'):\n",
    "    '''Returns pre computed entities for this docID, where each element is a tuple of (entity name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['entities']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['entities']\n",
    "    \n",
    "def retrieveEntityEmbedding(entity):\n",
    "    try:\n",
    "        return entityEmbeddingDict[entity]\n",
    "    except:\n",
    "        return  embed([entity])[0]\n",
    "    \n",
    "\n",
    "def computeSimilarityMatrix(query, text, method = 'title'):\n",
    "    _, queryEntitiesWithFreq = getQueryEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    _, docEntitiesWithFreq = getSpotsAndEntities(text)\n",
    "    docEntityFrequencies = [entityTuple[1] for entityTuple in docEntitiesWithFreq]\n",
    "    queryEntityFrequencies = [entityTuple[1] for entityTuple in queryEntitiesWithFreq]\n",
    "\n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityEmbeddings = embed(queryEntities)\n",
    "    docEntityEmbeddings = embed([preprocess(entityTuple[0]) for entityTuple in docEntitiesWithFreq])\n",
    "\n",
    "    \n",
    "    numDocEntities = len(docEntitiesWithFreq)\n",
    "    numQueryEntities = len(queryEntitiesWithFreq)\n",
    "    simMatrix = np.zeros((numDocEntities, numQueryEntities))\n",
    "    for i in range(numDocEntities):\n",
    "        for j in range(numQueryEntities):\n",
    "            simMatrix[i][j] = max(0, cosineSimilarity(docEntityEmbeddings[i], queryEntityEmbeddings[j]))\n",
    "    return simMatrix,  queryEntityFrequencies, docEntityFrequencies\n",
    "\n",
    "def reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = 'column', pooling = 'max'):\n",
    "    if axis == 'column':\n",
    "        axis = 0\n",
    "    else:\n",
    "        axis = 1\n",
    "    if pooling == 'max':\n",
    "        try:\n",
    "            return np.max(simMatrix, axis = axis) # along columns\n",
    "        except:\n",
    "            return np.zeros(1)\n",
    "    \n",
    "def reduceVector(vector, reduction = 'avg'):\n",
    "    if not  vector.size:\n",
    "        return 0\n",
    "    if reduction == 'avg':\n",
    "        return sum(vector) / len(vector)\n",
    "    \n",
    "def semanticScore(query, text, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeSimilarityMatrix(query, text, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5283d998a4097a31fa6f404a26377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'][0])\n",
    "        IDList.append(data['docno'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763049a1adbf44f685049bcf394323e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = dict()\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        records[data['docno']] = (data['title'][0], data['numCitedBy'][0], data['numKeyCitations'][0])\n",
    "        \n",
    "def getPaperDetails(docno):\n",
    "    return records[docno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_query.json') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        queryList.append(data['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e309d672d643d2a0ba77289ed2c1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queryEntityList = []\n",
    "for query in tqdm(queryList):\n",
    "    _, queryEntitiesWithFreq = getQueryEntitiesAndSpots(query, long_text = 0) \n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityList.append(queryEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f6a6a4124140cea25ac203ec185b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titleList = []\n",
    "citationList =  []\n",
    "keyCitationList = []\n",
    "queryIDList = []\n",
    "Y = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2.qrel') as file:\n",
    "    for line in tqdm(file):\n",
    "        lineString = line.split()\n",
    "        qid = int(lineString[0]) - 1    # - 1 to account for our 0 based indexing, while theirs is 1 based\n",
    "        docno = lineString[2]\n",
    "        relScore = int(lineString[-1])\n",
    "        if docno in records:\n",
    "            title, citations, keyCitations = records[docno]\n",
    "            queryIDList.append(qid)\n",
    "            titleList.append(title)\n",
    "            citationList.append(citations)\n",
    "            keyCitationList.append(keyCitations)\n",
    "            Y.append(relScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56bf512579d4b3b811b0dc71896b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "semScores = []\n",
    "for qid, title in tqdm(zip(queryIDList, titleList)):\n",
    "    semScores.append(semanticScore(queryList[qid], title, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## semScores citationList and keyCitationList are the feature vectors, Y to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for score, citation, keyCitation in zip(semScores, citationList, keyCitationList):\n",
    "    X.append([score, citation, keyCitation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Explicit_Semantic_Ranking_Dataset/trainData.json', 'w') as outfile:\n",
    "    tmp = dict()\n",
    "    tmp['X'] = X\n",
    "    tmp['Y'] = Y\n",
    "    json.dump(tmp, outfile)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/trainData.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        X = data['X']\n",
    "        Y = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "X = mm_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "names = [\n",
    " \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Linear SVC\" ]\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(verbose=True, n_jobs = -1),\n",
    "    MLPClassifier(verbose=True, early_stopping=True),\n",
    "    AdaBoostClassifier(),\n",
    "    OneVsRestClassifier(BaggingClassifier(LinearSVC(),n_jobs = -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  1\n",
      "Fitting:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  2\n",
      "Fitting:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  3\n",
      "Fitting:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  4\n",
      "Fitting:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  5\n",
      "Name Random Forest . Avg Precision:  0.4543500384048433 . Avg Recall:  0.5083833085477822 . Avg F-1 Score:  0.4718505867079064\n",
      "Fitting:  1\n",
      "Iteration 1, loss = inf\n",
      "Validation score: 0.251773\n",
      "Iteration 2, loss = 7.55720958\n",
      "Validation score: 0.546099\n",
      "Iteration 3, loss = 5.92031146\n",
      "Validation score: 0.340426\n",
      "Iteration 4, loss = 2.79012216\n",
      "Validation score: 0.503546\n",
      "Iteration 5, loss = 2.40280621\n",
      "Validation score: 0.514184\n",
      "Iteration 6, loss = 1.72917018\n",
      "Validation score: 0.553191\n",
      "Iteration 7, loss = 1.42025555\n",
      "Validation score: 0.453901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.40104929\n",
      "Validation score: 0.507092\n",
      "Iteration 9, loss = 1.48958578\n",
      "Validation score: 0.510638\n",
      "Iteration 10, loss = 1.29540101\n",
      "Validation score: 0.556738\n",
      "Iteration 11, loss = 1.36199916\n",
      "Validation score: 0.517730\n",
      "Iteration 12, loss = 1.28004453\n",
      "Validation score: 0.560284\n",
      "Iteration 13, loss = 1.30965289\n",
      "Validation score: 0.503546\n",
      "Iteration 14, loss = 1.24674873\n",
      "Validation score: 0.489362\n",
      "Iteration 15, loss = 1.25831572\n",
      "Validation score: 0.531915\n",
      "Iteration 16, loss = 1.21208737\n",
      "Validation score: 0.553191\n",
      "Iteration 17, loss = 1.25130266\n",
      "Validation score: 0.468085\n",
      "Iteration 18, loss = 1.32752573\n",
      "Validation score: 0.556738\n",
      "Iteration 19, loss = 1.20412401\n",
      "Validation score: 0.556738\n",
      "Iteration 20, loss = 1.28067435\n",
      "Validation score: 0.574468\n",
      "Iteration 21, loss = 1.23103942\n",
      "Validation score: 0.553191\n",
      "Iteration 22, loss = 1.25470570\n",
      "Validation score: 0.496454\n",
      "Iteration 23, loss = 1.32698958\n",
      "Validation score: 0.549645\n",
      "Iteration 24, loss = 1.35811087\n",
      "Validation score: 0.446809\n",
      "Iteration 25, loss = 1.32987275\n",
      "Validation score: 0.429078\n",
      "Iteration 26, loss = 1.36087634\n",
      "Validation score: 0.556738\n",
      "Iteration 27, loss = 1.30752031\n",
      "Validation score: 0.556738\n",
      "Iteration 28, loss = 1.36276323\n",
      "Validation score: 0.556738\n",
      "Iteration 29, loss = 1.43651621\n",
      "Validation score: 0.556738\n",
      "Iteration 30, loss = 1.44394222\n",
      "Validation score: 0.521277\n",
      "Iteration 31, loss = 1.37710415\n",
      "Validation score: 0.567376\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "count  1\n",
      "Fitting:  2\n",
      "Iteration 1, loss = inf\n",
      "Validation score: 0.407801\n",
      "Iteration 2, loss = 3.32910812\n",
      "Validation score: 0.514184\n",
      "Iteration 3, loss = 2.24920858\n",
      "Validation score: 0.361702\n",
      "Iteration 4, loss = 1.74395554\n",
      "Validation score: 0.482270\n",
      "Iteration 5, loss = 1.64896887\n",
      "Validation score: 0.453901\n",
      "Iteration 6, loss = 1.53506562\n",
      "Validation score: 0.468085\n",
      "Iteration 7, loss = 1.42837163\n",
      "Validation score: 0.464539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.55942045\n",
      "Validation score: 0.510638\n",
      "Iteration 9, loss = 1.31639635\n",
      "Validation score: 0.549645\n",
      "Iteration 10, loss = 1.34548840\n",
      "Validation score: 0.546099\n",
      "Iteration 11, loss = 1.38888211\n",
      "Validation score: 0.507092\n",
      "Iteration 12, loss = 1.31206225\n",
      "Validation score: 0.528369\n",
      "Iteration 13, loss = 1.38432008\n",
      "Validation score: 0.507092\n",
      "Iteration 14, loss = 1.26879006\n",
      "Validation score: 0.553191\n",
      "Iteration 15, loss = 1.27050075\n",
      "Validation score: 0.549645\n",
      "Iteration 16, loss = 1.35947195\n",
      "Validation score: 0.478723\n",
      "Iteration 17, loss = 1.38938739\n",
      "Validation score: 0.539007\n",
      "Iteration 18, loss = 1.42779408\n",
      "Validation score: 0.553191\n",
      "Iteration 19, loss = 1.56977012\n",
      "Validation score: 0.549645\n",
      "Iteration 20, loss = 2.02541527\n",
      "Validation score: 0.361702\n",
      "Iteration 21, loss = 1.60466002\n",
      "Validation score: 0.500000\n",
      "Iteration 22, loss = 1.43753049\n",
      "Validation score: 0.539007\n",
      "Iteration 23, loss = 1.27437860\n",
      "Validation score: 0.528369\n",
      "Iteration 24, loss = 1.29087814\n",
      "Validation score: 0.553191\n",
      "Iteration 25, loss = 1.26375290\n",
      "Validation score: 0.492908\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "count  2\n",
      "Fitting:  3\n",
      "Iteration 1, loss = inf\n",
      "Validation score: 0.297872\n",
      "Iteration 2, loss = inf\n",
      "Validation score: 0.269504\n",
      "Iteration 3, loss = 3.44511026\n",
      "Validation score: 0.379433\n",
      "Iteration 4, loss = 2.26112385\n",
      "Validation score: 0.336879\n",
      "Iteration 5, loss = 1.88574689\n",
      "Validation score: 0.485816\n",
      "Iteration 6, loss = 1.73630323\n",
      "Validation score: 0.503546\n",
      "Iteration 7, loss = 1.59507229\n",
      "Validation score: 0.514184\n",
      "Iteration 8, loss = 1.58078578\n",
      "Validation score: 0.531915\n",
      "Iteration 9, loss = 1.46565526\n",
      "Validation score: 0.549645\n",
      "Iteration 10, loss = 1.54114656\n",
      "Validation score: 0.521277\n",
      "Iteration 11, loss = 1.51845059\n",
      "Validation score: 0.535461\n",
      "Iteration 12, loss = 1.49893188\n",
      "Validation score: 0.535461\n",
      "Iteration 13, loss = 1.48139272\n",
      "Validation score: 0.507092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 1.31422661\n",
      "Validation score: 0.514184\n",
      "Iteration 15, loss = 1.37623651\n",
      "Validation score: 0.542553\n",
      "Iteration 16, loss = 1.34076714\n",
      "Validation score: 0.539007\n",
      "Iteration 17, loss = 1.45086597\n",
      "Validation score: 0.556738\n",
      "Iteration 18, loss = 1.34954359\n",
      "Validation score: 0.553191\n",
      "Iteration 19, loss = 1.38979100\n",
      "Validation score: 0.521277\n",
      "Iteration 20, loss = 1.43792396\n",
      "Validation score: 0.443262\n",
      "Iteration 21, loss = 1.40911255\n",
      "Validation score: 0.539007\n",
      "Iteration 22, loss = 1.39495887\n",
      "Validation score: 0.553191\n",
      "Iteration 23, loss = 1.22858851\n",
      "Validation score: 0.556738\n",
      "Iteration 24, loss = 1.22353711\n",
      "Validation score: 0.503546\n",
      "Iteration 25, loss = 1.36109716\n",
      "Validation score: 0.556738\n",
      "Iteration 26, loss = 1.57559025\n",
      "Validation score: 0.450355\n",
      "Iteration 27, loss = 1.40988502\n",
      "Validation score: 0.542553\n",
      "Iteration 28, loss = 1.44468334\n",
      "Validation score: 0.546099\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "count  3\n",
      "Fitting:  4\n",
      "Iteration 1, loss = inf\n",
      "Validation score: 0.432624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 5.84561753\n",
      "Validation score: 0.531915\n",
      "Iteration 3, loss = 4.67587797\n",
      "Validation score: 0.468085\n",
      "Iteration 4, loss = 3.14843456\n",
      "Validation score: 0.404255\n",
      "Iteration 5, loss = 2.09158184\n",
      "Validation score: 0.489362\n",
      "Iteration 6, loss = 1.89343067\n",
      "Validation score: 0.581560\n",
      "Iteration 7, loss = 1.83432283\n",
      "Validation score: 0.485816\n",
      "Iteration 8, loss = 1.68983197\n",
      "Validation score: 0.521277\n",
      "Iteration 9, loss = 1.53681557\n",
      "Validation score: 0.563830\n",
      "Iteration 10, loss = 1.41095601\n",
      "Validation score: 0.546099\n",
      "Iteration 11, loss = 1.31538008\n",
      "Validation score: 0.592199\n",
      "Iteration 12, loss = 1.40162333\n",
      "Validation score: 0.595745\n",
      "Iteration 13, loss = 1.61374687\n",
      "Validation score: 0.411348\n",
      "Iteration 14, loss = 1.50499774\n",
      "Validation score: 0.443262\n",
      "Iteration 15, loss = 1.43038855\n",
      "Validation score: 0.570922\n",
      "Iteration 16, loss = 1.38090427\n",
      "Validation score: 0.546099\n",
      "Iteration 17, loss = 1.39145766\n",
      "Validation score: 0.517730\n",
      "Iteration 18, loss = 1.34181574\n",
      "Validation score: 0.581560\n",
      "Iteration 19, loss = 1.24543108\n",
      "Validation score: 0.585106\n",
      "Iteration 20, loss = 1.26750831\n",
      "Validation score: 0.560284\n",
      "Iteration 21, loss = 1.25023350\n",
      "Validation score: 0.574468\n",
      "Iteration 22, loss = 1.39678562\n",
      "Validation score: 0.595745\n",
      "Iteration 23, loss = 1.37608246\n",
      "Validation score: 0.585106\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "count  4\n",
      "Fitting:  5\n",
      "Iteration 1, loss = inf\n",
      "Validation score: 0.400709\n",
      "Iteration 2, loss = inf\n",
      "Validation score: 0.574468\n",
      "Iteration 3, loss = 6.58246276\n",
      "Validation score: 0.443262\n",
      "Iteration 4, loss = 3.19239697\n",
      "Validation score: 0.450355\n",
      "Iteration 5, loss = 2.53769226\n",
      "Validation score: 0.418440\n",
      "Iteration 6, loss = 1.99735604\n",
      "Validation score: 0.574468\n",
      "Iteration 7, loss = 1.72408476\n",
      "Validation score: 0.471631\n",
      "Iteration 8, loss = 1.47686861\n",
      "Validation score: 0.542553\n",
      "Iteration 9, loss = 1.58228667\n",
      "Validation score: 0.492908\n",
      "Iteration 10, loss = 1.63528400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.574468\n",
      "Iteration 11, loss = 1.39867342\n",
      "Validation score: 0.489362\n",
      "Iteration 12, loss = 1.41331768\n",
      "Validation score: 0.478723\n",
      "Iteration 13, loss = 1.29936838\n",
      "Validation score: 0.503546\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "count  5\n",
      "Name Neural Net . Avg Precision:  0.40207363787293815 . Avg Recall:  0.5626668983576877 . Avg F-1 Score:  0.4300249990181776\n",
      "Fitting:  1\n",
      "count  1\n",
      "Fitting:  2\n",
      "count  2\n",
      "Fitting:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  3\n",
      "Fitting:  4\n",
      "count  4\n",
      "Fitting:  5\n",
      "count  5\n",
      "Name AdaBoost . Avg Precision:  0.4133169652514864 . Avg Recall:  0.5629380576748998 . Avg F-1 Score:  0.4353654910203583\n",
      "Fitting:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  1\n",
      "Fitting:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  2\n",
      "Fitting:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  3\n",
      "Fitting:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  4\n",
      "Fitting:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  5\n",
      "Name Linear SVC . Avg Precision:  0.41553591404060075 . Avg Recall:  0.4836480020690548 . Avg F-1 Score:  0.4135348188974411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def average(lis):\n",
    "    return sum(lis) / len(lis)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for name, clf in zip(names, classifiers):\n",
    "    precScores = []\n",
    "    recallScores = []\n",
    "    f1Scores = []\n",
    "    count = 1\n",
    "    for train_index, test_index in kfold.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        print('Fitting: ', count)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('count ', count)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        prec, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        precScores.append(prec)\n",
    "        recallScores.append(recall)\n",
    "        f1Scores.append(fscore)\n",
    "        count += 1\n",
    "    print('Name', name,'. Avg Precision: ', average(precScores), '. Avg Recall: ', average(recallScores), '. Avg F-1 Score: ', average(f1Scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other similairity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a57ecf7ba04f4a82512b78dd1228ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "fuzzyScores = []\n",
    "for qid, title in tqdm(zip(queryIDList, titleList)):\n",
    "    fuzzyScores.append(fuzz.token_sort_ratio(queryList[qid], title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for score, citation, keyCitation in zip(fuzzyScores, citationList, keyCitationList):\n",
    "    X.append([score, citation, keyCitation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
