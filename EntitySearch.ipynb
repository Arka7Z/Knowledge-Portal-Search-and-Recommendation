{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import re \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityEmbeddingDict = dict()\n",
    "with open('./data/entity_USE_Embeddings.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        entityEmbeddingDict[data['entity']] = data['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def cosineSimilarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getEntitiesAndSpots(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'true', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "         \n",
    "dictForTitles = dict()\n",
    "dictForBody = dict()\n",
    "titleDictLoaded = False\n",
    "bodyDictLoaded = False\n",
    "def loadEntityDict(method='title'):\n",
    "    global dictForTitles\n",
    "    global dictForBody\n",
    "    if method == 'title':\n",
    "        with open('./data/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForTitles = json.loads(line)\n",
    "    elif method == 'body':\n",
    "        with open('./data/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForBody = json.loads(line)\n",
    "def retrieveSpots(docID, method='title'):\n",
    "    '''Returns pre computed spot mentions for this docID, where each element is a tuple of (spot name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['spots']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['spots']\n",
    "    \n",
    "def retrieveEntities(docID, method='title'):\n",
    "    '''Returns pre computed entities for this docID, where each element is a tuple of (entity name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['entities']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['entities']\n",
    "    \n",
    "def retrieveEntityEmbedding(entity):\n",
    "    try:\n",
    "        return entityEmbeddingDict[entity]\n",
    "    except:\n",
    "        return  embed([entity]).numpy().tolist()[0]\n",
    "    \n",
    "def computeFuzzySimilarityMatrix(query, docID, method = 'title'):\n",
    "    querySpotsWithFreq, _ = getEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    docSpotsWithFreq = retrieveSpots(docID, method = method)\n",
    "    docSpotFrequencies = [entityTuple[1] for entityTuple in docSpotsWithFreq]\n",
    "    querySpotFrequencies = [entityTuple[1] for entityTuple in querySpotsWithFreq]\n",
    "\n",
    "    querySpots = []\n",
    "    for entityTuple in querySpotsWithFreq:\n",
    "        querySpots.append(preprocess(entityTuple[0]))\n",
    "    docSpots = [preprocess(entityTuple[0]) for entityTuple in docSpotsWithFreq]\n",
    "\n",
    "    numDocSpots = len(docSpotsWithFreq)\n",
    "    numQuerySpots = len(querySpotsWithFreq)\n",
    "    simMatrix = np.zeros((numDocSpots, numQuerySpots))\n",
    "    for i in range(numDocSpots):\n",
    "        for j in range(numQuerySpots):\n",
    "            simMatrix[i][j] = fuzz.token_sort_ratio(docSpots[i], querySpots[j])\n",
    "    return simMatrix,  querySpotFrequencies, docSpotFrequencies\n",
    "\n",
    "def computeSimilarityMatrix(query, docID, method = 'title'):\n",
    "    _, queryEntitiesWithFreq = getEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    docEntitiesWithFreq = retrieveEntities(docID, method = method)\n",
    "    docEntityFrequencies = [entityTuple[1] for entityTuple in docEntitiesWithFreq]\n",
    "    queryEntityFrequencies = [entityTuple[1] for entityTuple in queryEntitiesWithFreq]\n",
    "\n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityEmbeddings = embed(queryEntities).numpy().tolist()\n",
    "    docEntityEmbeddings = [retrieveEntityEmbedding(entityTuple[0]) for entityTuple in docEntitiesWithFreq]\n",
    "    \n",
    "    numDocEntities = len(docEntitiesWithFreq)\n",
    "    numQueryEntities = len(queryEntitiesWithFreq)\n",
    "    simMatrix = np.zeros((numDocEntities, numQueryEntities))\n",
    "    for i in range(numDocEntities):\n",
    "        for j in range(numQueryEntities):\n",
    "            simMatrix[i][j] = max(0, cosineSimilarity(docEntityEmbeddings[i], queryEntityEmbeddings[j]))\n",
    "    return simMatrix,  queryEntityFrequencies, docEntityFrequencies\n",
    "\n",
    "def reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = 'column', pooling = 'max'):\n",
    "    if axis == 'column':\n",
    "        axis = 0\n",
    "    else:\n",
    "        axis = 1\n",
    "    if pooling == 'max':\n",
    "        try:\n",
    "            return np.max(simMatrix, axis = axis) # along columns\n",
    "        except:\n",
    "            return np.zeros(1)\n",
    "    \n",
    "def reduceVector(vector, reduction = 'avg'):\n",
    "    if reduction == 'avg':\n",
    "        return sum(vector) / len(vector)\n",
    "    \n",
    "def semanticScore(query, docID, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeSimilarityMatrix(query, docID, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "def fuzzyScore(query, docID, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeFuzzySimilarityMatrix(query, docID, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "def search(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScores = [ semanticScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "\n",
    "def searchFuzzy(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScores = [ fuzzyScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n",
    "\n",
    "def averageScores(scores1, scores2):\n",
    "    array_1 = np.array(scores1)\n",
    "    array_2 = np.array(scores2)\n",
    "\n",
    "    weight_1 = 0.3\n",
    "    weight_2 = 0.7\n",
    "    meanArray = weight_1*array_1 + weight_2*array_2\n",
    "    return meanArray.tolist()\n",
    "\n",
    "def searchFusion(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScoresFuzzy = [ fuzzyScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    simScoresFuzzy = normalize(simScoresFuzzy)\n",
    "    simScoresSem = [ semanticScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    simScores = averageScores(simScoresFuzzy, simScoresSem)\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/papersForEntity.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "embeddingResults = data['embeddingResults']\n",
    "esResults = data['esResults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['converting text to speech', 'big data', 'efficient estimation of word representations in vector space', 'natural language interface', 'reinforcement learning in video game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ES ID List input, semantic search, title\n",
    "results1 = []\n",
    "for i in range(len(queryList)):\n",
    "    results1.append(search(query, esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding ID List input, semantic search, title\n",
    "results2 = []\n",
    "for i in range(len(queryList)):\n",
    "    results2.append(search(query, embeddingResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, semantic search, body\n",
    "results3 = []\n",
    "for i in range(len(queryList)):\n",
    "    results3.append(search(query, esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fuzzy search, body\n",
    "results4 = []\n",
    "for i in range(len(queryList)):\n",
    "    results4.append(searchFuzzy(query, esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.7, title\n",
    "results5 = []\n",
    "for i in range(len(queryList)):\n",
    "    results5.append(searchFusion(query, esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.7, body\n",
    "results6 = []\n",
    "for i in range(len(queryList)):\n",
    "    results6.append(searchFusion(query, esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.5, body\n",
    "results6 = []\n",
    "for i in range(len(queryList)):\n",
    "    results6.append(searchFusion(query, esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6876e7434b564344a92d1d2ed20d18df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'])\n",
    "        IDList.append(data['id'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scribe: deep integration of human and machine intelligence to caption speech in real time',\n",
       " 'Generating expressive speech for storytelling applications',\n",
       " 'Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech',\n",
       " 'Converting dependency structures to phrase structures',\n",
       " 'An unrestricted vocabulary Arabic speech synthesis system',\n",
       " 'Visual signals in text comprehension: How to restore them when oralizing a text via a speech synthesis?',\n",
       " 'Towards speech-to-text translation without speech recognition.',\n",
       " 'Quaero Speech-to-Text and Text Translation Evaluation Systems',\n",
       " 'Text-to-speech conversion technology',\n",
       " 'Use of text syntactical structures in detection of document duplicates',\n",
       " 'Segmenting unrestricted Chinese text into prosodic words instead of lexical words']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(ID) for ID in tmpResults]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'text to speech'\n",
    "tmpResults = search(query, embeddingResults[0], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = []\n",
    "for i in range(len(queryList)):\n",
    "    results3.append(search(query, esResults[i], K = 10, method = 'body', axis = 'row', pooling = 'max', reduction = 'avg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deriving Adjectival Scales from Continuous Space Word Representations',\n",
       " 'Improving Vector Space Word Representations Using Multilingual Correlation',\n",
       " 'Vector Space Representations of Documents in Classifying Finnish Social Media Texts',\n",
       " 'Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models',\n",
       " 'Multi-Prototype Vector-Space Models of Word Meaning',\n",
       " 'Co-learning of Word Representations and Morpheme Representations',\n",
       " 'PART-OF-SPEECH INDUCTION FROM SCRATCH',\n",
       " 'A Structured Vector Space Model for Word Meaning in Context',\n",
       " 'Non-distributional Word Vector Representations',\n",
       " 'Modelling Word Meaning using Efficient Tensor Representations']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpResults = results3[2]\n",
    "[ret(ID) for ID in tmpResults]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multi-objective tree search approaches for general video game playing',\n",
       " 'The Reinforcement Learning Competition 2014',\n",
       " 'Learning to compete, compromise, and cooperate in repeated general-sum games',\n",
       " 'ViZDoom: A Doom-based AI research platform for visual reinforcement learning',\n",
       " 'An object-oriented approach to reinforcement learning in an action game',\n",
       " 'Robust, Efficient, Globally-Optimized Reinforcement Learning with the Parti-Game Algorithm',\n",
       " 'Learning character behaviors using agent modeling in games',\n",
       " 'Deep Learning for Video Game Playing.',\n",
       " 'Video summarization using reinforcement learning in eigenspace',\n",
       " 'Learning and knowledge generation in General Games']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(ID) for ID in tmpResults]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VECTOR SPACE MODELS OF WORD MEANING AND PHRASE MEANING: A SURVEY',\n",
       " 'PART-OF-SPEECH INDUCTION FROM SCRATCH',\n",
       " 'Vector Space Representations of Documents in Classifying Finnish Social Media Texts',\n",
       " 'Improving Vector Space Word Representations Using Multilingual Correlation',\n",
       " 'A Generative Model of Vector Space Semantics',\n",
       " 'Word Representations in Vector Space and their Applications for Arabic',\n",
       " 'Modeling N400 amplitude using vector space models of word representation.',\n",
       " 'Efficient Estimation of Word Representations in Vector Space',\n",
       " 'A Structured Vector Space Model for Word Meaning in Context',\n",
       " 'From Word to Sense Embeddings: A Survey on Vector Representations of Meaning.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(ID) for ID in results5[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutlIDList = []\n",
    "for r1, r2, r3, r4, r5 in zip(results1, results2, results3, results4, results5):\n",
    "    lis = list(set().union(r1, r2, r3, r4, r5))\n",
    "    resutlIDList.append(lis)\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "IDs = set(reduce(operator.concat, resutlIDList))\n",
    "\n",
    "PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "i = 0\n",
    "records = dict()\n",
    "with open(PapersOutFileName, 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        if i % 2 != 0:\n",
    "            data = json.loads(line)\n",
    "            if (data['id'] in IDs):\n",
    "                records[data['id']] = {'title': data['title'], 'abstract': data['abstract'], 'fos': ', '.join(data['fos'])}\n",
    "        i += 1\n",
    "        \n",
    "import random\n",
    "\n",
    "rows = []\n",
    "for query, idSubList in tqdm(zip(queryList, resutlIDList)):\n",
    "    for ID in idSubList:\n",
    "        localDict = records[ID]\n",
    "        rows.append([query, ID, localDict['title'], localDict['abstract'], localDict['fos'], 0])\n",
    "random.shuffle(rows)\n",
    "        \n",
    "with open('./data/entityAnnotations.csv', \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['query', 'id', 'title', 'abstract', 'fos', 'score']\n",
    "    writer.writerow(header)\n",
    "    for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "results = [results1, results2, results3, results4, results5]\n",
    "len(results)\n",
    "\n",
    "with open('./data/entity_search_results.json', 'w') as outfile:\n",
    "    for i in tqdm(range(len(results))):\n",
    "        outDict = dict()\n",
    "        outDict['id'] = i\n",
    "        outDict['result'] = results[i]\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
