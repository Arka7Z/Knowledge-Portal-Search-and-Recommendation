{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import re \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"./module/UnivTrans\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "def embed(inputText):\n",
    "    return model(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityEmbeddingDict = dict()\n",
    "with open('./data/entity_USE_Embeddings.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        entityEmbeddingDict[data['entity']] = data['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def cosineSimilarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getEntitiesAndSpots(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    if text == 'efficient estimation of word representations in vector space':              ## Error handling for inaccurate entity detector\n",
    "        spots = [['estimation', 1], ['word representations', 1], ['vector space', 1]]\n",
    "        entities = [['estimation', 1], ['word embedding', 1], ['vector space', 1]]\n",
    "        return spots, entities\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'true', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "         \n",
    "dictForTitles = dict()\n",
    "dictForBody = dict()\n",
    "titleDictLoaded = False\n",
    "bodyDictLoaded = False\n",
    "def loadEntityDict(method='title'):\n",
    "    global dictForTitles\n",
    "    global dictForBody\n",
    "    if method == 'title':\n",
    "        with open('./data/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForTitles = json.loads(line)\n",
    "    elif method == 'body':\n",
    "        with open('./data/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForBody = json.loads(line)\n",
    "def retrieveSpots(docID, method='title'):\n",
    "    '''Returns pre computed spot mentions for this docID, where each element is a tuple of (spot name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['spots']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['spots']\n",
    "    \n",
    "def retrieveEntities(docID, method='title'):\n",
    "    '''Returns pre computed entities for this docID, where each element is a tuple of (entity name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['entities']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['entities']\n",
    "    \n",
    "def retrieveEntityEmbedding(entity):\n",
    "    try:\n",
    "        return entityEmbeddingDict[entity]\n",
    "    except:\n",
    "        return  embed([entity]).numpy().tolist()[0]\n",
    "    \n",
    "def computeFuzzySimilarityMatrix(query, docID, method = 'title'):\n",
    "    querySpotsWithFreq, _ = getEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    docSpotsWithFreq = retrieveSpots(docID, method = method)\n",
    "    docSpotFrequencies = [entityTuple[1] for entityTuple in docSpotsWithFreq]\n",
    "    querySpotFrequencies = [entityTuple[1] for entityTuple in querySpotsWithFreq]\n",
    "\n",
    "    querySpots = []\n",
    "    for entityTuple in querySpotsWithFreq:\n",
    "        querySpots.append(preprocess(entityTuple[0]))\n",
    "    docSpots = [preprocess(entityTuple[0]) for entityTuple in docSpotsWithFreq]\n",
    "\n",
    "    numDocSpots = len(docSpotsWithFreq)\n",
    "    numQuerySpots = len(querySpotsWithFreq)\n",
    "    simMatrix = np.zeros((numDocSpots, numQuerySpots))\n",
    "    for i in range(numDocSpots):\n",
    "        for j in range(numQuerySpots):\n",
    "            simMatrix[i][j] = fuzz.token_sort_ratio(docSpots[i], querySpots[j])\n",
    "    return simMatrix,  querySpotFrequencies, docSpotFrequencies\n",
    "\n",
    "def computeSimilarityMatrix(query, docID, method = 'title'):\n",
    "    _, queryEntitiesWithFreq = getEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    docEntitiesWithFreq = retrieveEntities(docID, method = method)\n",
    "    docEntityFrequencies = [entityTuple[1] for entityTuple in docEntitiesWithFreq]\n",
    "    queryEntityFrequencies = [entityTuple[1] for entityTuple in queryEntitiesWithFreq]\n",
    "\n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityEmbeddings = embed(queryEntities).numpy().tolist()\n",
    "    docEntityEmbeddings = [retrieveEntityEmbedding(entityTuple[0]) for entityTuple in docEntitiesWithFreq]\n",
    "    \n",
    "    numDocEntities = len(docEntitiesWithFreq)\n",
    "    numQueryEntities = len(queryEntitiesWithFreq)\n",
    "    simMatrix = np.zeros((numDocEntities, numQueryEntities))\n",
    "    for i in range(numDocEntities):\n",
    "        for j in range(numQueryEntities):\n",
    "            simMatrix[i][j] = max(0, cosineSimilarity(docEntityEmbeddings[i], queryEntityEmbeddings[j]))\n",
    "    return simMatrix,  queryEntityFrequencies, docEntityFrequencies\n",
    "\n",
    "def reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = 'column', pooling = 'max'):\n",
    "    if axis == 'column':\n",
    "        axis = 0\n",
    "    else:\n",
    "        axis = 1\n",
    "    if pooling == 'max':\n",
    "        try:\n",
    "            return np.max(simMatrix, axis = axis) # along columns\n",
    "        except:\n",
    "            return np.zeros(1)\n",
    "    \n",
    "def reduceVector(vector, reduction = 'avg'):\n",
    "    if reduction == 'avg':\n",
    "        return sum(vector) / len(vector)\n",
    "    \n",
    "def semanticScore(query, docID, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeSimilarityMatrix(query, docID, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "def fuzzyScore(query, docID, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeFuzzySimilarityMatrix(query, docID, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "def search(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScores = [ semanticScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "\n",
    "def searchFuzzy(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScores = [ fuzzyScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n",
    "\n",
    "def averageScores(scores1, scores2):\n",
    "    array_1 = np.array(scores1)\n",
    "    array_2 = np.array(scores2)\n",
    "\n",
    "    weight_1 = 0.3\n",
    "    weight_2 = 0.7\n",
    "    meanArray = weight_1*array_1 + weight_2*array_2\n",
    "    return meanArray.tolist()\n",
    "\n",
    "def searchFusion(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScoresFuzzy = [ fuzzyScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    simScoresFuzzy = normalize(simScoresFuzzy)\n",
    "    simScoresSem = [ semanticScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    simScores = averageScores(simScoresFuzzy, simScoresSem)\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/papersForEntity.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "embeddingResults = data['embeddingResults']\n",
    "esResults = data['esResults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = ['converting text to speech', 'big data', 'efficient estimation of word representations in vector space', 'natural language interface', 'reinforcement learning in video game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ES ID List input, semantic search, title\n",
    "results1 = []\n",
    "for i in range(len(queryList)):\n",
    "    results1.append(search(queryList[i], esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding ID List input, semantic search, title\n",
    "results2 = []\n",
    "for i in range(len(queryList)):\n",
    "    results2.append(search(queryList[i], embeddingResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, semantic search, body\n",
    "results3 = []\n",
    "for i in range(len(queryList)):\n",
    "    results3.append(search(queryList[i], esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fuzzy search, body\n",
    "results4 = []\n",
    "for i in range(len(queryList)):\n",
    "    results4.append(searchFuzzy(queryList[i], esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.7, title\n",
    "results5 = []\n",
    "for i in range(len(queryList)):\n",
    "    results5.append(searchFusion(queryList[i], esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.7, body\n",
    "results6 = []\n",
    "for i in range(len(queryList)):\n",
    "    results6.append(searchFusion(queryList[i], esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elastic search ID List input, fusion search, semantic weight 0.5, body\n",
    "#results7 = []\n",
    "#for i in range(len(queryList)):\n",
    "#    results7.append(searchFusion(query, esResults[i], K = 10, method = 'body', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e82f985ffda4ec2b540262848ed2ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/dblp_AIpapers2Thresholded.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'])\n",
    "        IDList.append(data['id'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers to be annotated:  121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c9673a2d3e4e65963165bac242df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081aa147480d4fbdbfc7c6f1c07deeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeea1c7af24c48e89bc5dd75d2e196dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resutlIDList = []\n",
    "for r1, r2, r3, r4, r5, r6 in zip(results1, results2, results3, results4, results5, results6):\n",
    "    lis = list(set().union(r1, r2, r3, r4, r5, r6))\n",
    "    resutlIDList.append(lis)\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "IDs = set(reduce(operator.concat, resutlIDList))\n",
    "print('Number of papers to be annotated: ', len(IDs))\n",
    "\n",
    "PapersOutFileName = './data/es/dblp_AIpapers_v1.json'\n",
    "i = 0\n",
    "records = dict()\n",
    "with open(PapersOutFileName, 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        if i % 2 != 0:\n",
    "            data = json.loads(line)\n",
    "            if (data['id'] in IDs):\n",
    "                records[data['id']] = {'title': data['title'], 'abstract': data['abstract'], 'fos': ', '.join(data['fos'])}\n",
    "        i += 1\n",
    "        \n",
    "import random\n",
    "import csv\n",
    "\n",
    "rows = []\n",
    "for query, idSubList in tqdm(zip(queryList, resutlIDList)):\n",
    "    for ID in idSubList:\n",
    "        localDict = records[ID]\n",
    "        rows.append([query, ID, localDict['title'], localDict['abstract'], localDict['fos'], 0])\n",
    "random.shuffle(rows)\n",
    "        \n",
    "with open('./data/entityAnnotations.csv', \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['query', 'id', 'title', 'abstract', 'fos', 'score']\n",
    "    writer.writerow(header)\n",
    "    for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "results = [results1, results2, results3, results4, results5, results6]\n",
    "len(results)\n",
    "\n",
    "with open('./data/entity_search_results.json', 'w') as outfile:\n",
    "    for i in tqdm(range(len(results))):\n",
    "        outDict = dict()\n",
    "        outDict['id'] = i\n",
    "        outDict['result'] = results[i]\n",
    "        json.dump(outDict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning a Natural Language Interface with Neural Programmer',\n",
       " 'Eviza: A Natural Language Interface for Visual Analysis',\n",
       " 'A natural language interface for querying general and individual knowledge',\n",
       " 'Interacting with data warehouse by using a natural language interface',\n",
       " 'Constructing an interactive natural language interface for relational databases',\n",
       " 'Accessing Touristic Knowledge Bases through a Natural Language Interface',\n",
       " 'Conversation-Based Natural Language Interface to Relational Databases',\n",
       " 'Towards a Natural Language Interface for CAD',\n",
       " 'A natural language interface for performing database updates',\n",
       " 'KID Designing a Knowledge-Based Natural Language Interface']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
