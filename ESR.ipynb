{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dab85b7cda4a0fb24932bffffbb4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        tmp = dict()\n",
    "        tmp['keyPhrases'] = data.get('keyPhrases', [])\n",
    "        tmp['paperAbstract'] = data['paperAbstract'][0]\n",
    "        tmp['title'] = data['title'][0]\n",
    "        tmp['id'] = data['docno']\n",
    "        records.append(tmp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyPhrases': ['Isomorphism',\n",
       "  'Duality',\n",
       "  'DUAL',\n",
       "  'Undirected Graph',\n",
       "  'Relational'],\n",
       " 'paperAbstract': \"We provide a correspondence between the subjects of duality and density in classes of finite relational structures. The purpose of du-ality is to characterise the structures C that do not admit a homo-morphism into a given target B by the existence of a homomorphism from a structure A into C. Density is the order-theoretic property of containing no covers (or 'gaps'). We show that the covers in the skeleton of a category of finite relational models correspond naturally to certain instances of duality statements, and we characterise these covers.\",\n",
       " 'title': 'Duality Theorems for Finite Structures (Characterising Gaps and Good Characterisations)',\n",
       " 'id': '6e4eddf4d6671c37537bb5d1c9623353b62e8531'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b320637280594635881e6b589e1311fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8541.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for record in tqdm(records):\n",
    "    res = es.index(index='esr',doc_type='paper',id=record['id'],body=record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    s = s.translate(translator) \n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def getSpotsAndEntities(text, rhoThreshold = 0.1, long_text = 0):\n",
    "    url = 'https://tagme.d4science.org/tagme/tag'\n",
    "    params = {'lang': 'en', 'include_abstract': 'false', 'include_categories': 'true', 'gcube-token': '42aa36f7-4770-4574-8ef8-45138f3ba072-843339462', 'text': text, 'long_text': long_text}\n",
    "    rhoThreshold = rhoThreshold\n",
    "    entities = []\n",
    "    spots = []\n",
    "    r = requests.get(url = url, params = params) \n",
    "    data = r.json()\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation['rho'] > rhoThreshold:\n",
    "            entities.append(annotation['title'])\n",
    "            spots.append(annotation['spot'])\n",
    "    spots = Counter(spots)\n",
    "    spots = [(s, spots[s]) for s in spots.keys()]\n",
    "    entities = Counter(entities)\n",
    "    entities = [(s, entities[s]) for s in entities.keys()]\n",
    "    return spots, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "def embed(inputTexts):\n",
    "    return model.encode(inputTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "def elasticSearch(queryList, index='esr', K = 10):\n",
    "    queryBody = {\n",
    "    \"query\": {\n",
    "        \"multi_match\" : {\n",
    "            \"query\" : \"sentence embeddings\",\n",
    "            \"fields\" : ['title', 'paperAbstract']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    queryBody['size'] = K\n",
    "    searchResults = []\n",
    "    for query in queryList:\n",
    "        queryBody['query']['multi_match']['query'] = query\n",
    "        res= es.search(index=index,body=queryBody)\n",
    "        searchResults.append([hit['_id'] for hit in res['hits']['hits']])\n",
    "    return searchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_query.json') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        queryList.append(data['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = queryList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "esResults = elasticSearch(queryList, K = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDict = dict()\n",
    "tmpDict['esResults'] = esResults\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/papersForEntity.json', 'w') as outfile:\n",
    "    json.dump(tmpDict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperIDs = set()\n",
    "for result in esResults:\n",
    "    paperIDs.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = dict()\n",
    "papersFileName = './data/Explicit_Semantic_Ranking_Dataset/s2_doc.json'\n",
    "with open(papersFileName, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        if (data['docno'] in paperIDs):\n",
    "            records[data['docno']] = {'title': data['title'][0], 'abstract': data['paperAbstract'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d48dd14eb554eb09051579926ab783e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=963.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictForTitles = dict() ## dict[paperId] = {'entities': entityCounterList,'spots': spotscounterList}\n",
    "for key in tqdm(records.keys()):\n",
    "    text = preprocess(records[key]['title'])\n",
    "    spots, entities = getSpotsAndEntities(text, rhoThreshold = 0.1)\n",
    "    dictForTitles[key] = {'entities': entities, 'spots': spots}\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/TitleEntitiesPerPaper.json', 'w') as outfile:\n",
    "    json.dump(dictForTitles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dictForTitles = json.loads(line)\n",
    "entityList = [[entityTuple[0] for entityTuple in tmpDict['entities']] for tmpDict in  dictForTitles.values() ]\n",
    "entitySet = set()\n",
    "for entitySubList in entityList:\n",
    "    entitySet.update(entitySubList)\n",
    "entityList = list(entitySet)\n",
    "\n",
    "n = 100     # block size\n",
    "entities = [entityList[i:i + n] for i in range(0, len(entityList), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/entity_Roberta_Embeddings.json', 'w') as outfile:\n",
    "    for entitySubList in tqdm(entities):\n",
    "        entitySubList = [preprocess(entity) for entity in entitySubList]\n",
    "        embeddings = embed(entitySubList)#.numpy().tolist()\n",
    "        for embedding, entity in zip(embeddings, entitySubList):\n",
    "            outDict = dict()\n",
    "            outDict['entity'] = entity\n",
    "            outDict['embedding'] = embedding.tolist()\n",
    "            count += 1\n",
    "            json.dump(outDict, outfile)\n",
    "            outfile.write('\\n')\n",
    "assert count == len(entityList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity -Similarity Matrix Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def l1similarity(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    return 1 / ( 1+ np.linalg.norm((a - b), ord=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictForTitles = dict()\n",
    "dictForBody = dict()\n",
    "titleDictLoaded = False\n",
    "bodyDictLoaded = False\n",
    "queryEntitySpotDict = dict()\n",
    "def getQueryEntitiesAndSpots(query, long_text = 0):\n",
    "    global queryEntitySpotDict \n",
    "    if query in queryEntitySpotDict:\n",
    "        return queryEntitySpotDict[query]['entities'], queryEntitySpotDict[query]['spots']\n",
    "    else:\n",
    "        spotsWithFreq, entitiesWithFreq = getSpotsAndEntities(query, long_text = 0)\n",
    "        queryEntitySpotDict[query]= {'entities': entitiesWithFreq,'spots' : spotsWithFreq}\n",
    "        return entitiesWithFreq, spotsWithFreq\n",
    "def loadEntityDict(method='title'):\n",
    "    global dictForTitles\n",
    "    global dictForBody\n",
    "    if method == 'title':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/TitleEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForTitles = json.loads(line)\n",
    "    elif method == 'body':\n",
    "        with open('./data/Explicit_Semantic_Ranking_Dataset/BodyEntitiesPerPaper.json', 'r') as file:\n",
    "            for line in file:\n",
    "                dictForBody = json.loads(line)\n",
    "def retrieveSpots(docID, method='title'):\n",
    "    '''Returns pre computed spot mentions for this docID, where each element is a tuple of (spot name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['spots']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['spots']\n",
    "    \n",
    "def retrieveEntities(docID, method='title'):\n",
    "    '''Returns pre computed entities for this docID, where each element is a tuple of (entity name, frequency)'''\n",
    "    global titleDictLoaded \n",
    "    global bodyDictLoaded\n",
    "    if titleDictLoaded == False and method == 'title':\n",
    "        loadEntityDict(method='title')\n",
    "        titleDictLoaded = True\n",
    "    elif bodyDictLoaded == False and method == 'body':\n",
    "        loadEntityDict(method='body')\n",
    "        bodyDictLoaded = True\n",
    "        \n",
    "    if method == 'title':\n",
    "        return dictForTitles[docID]['entities']\n",
    "    elif method == 'body':\n",
    "        return dictForBody[docID]['entities']\n",
    "    \n",
    "def retrieveEntityEmbedding(entity):\n",
    "    try:\n",
    "        return entityEmbeddingDict[entity]\n",
    "    except:\n",
    "        return  embed([entity])[0]\n",
    "    \n",
    "\n",
    "def computeSimilarityMatrix(query, docID, method = 'title'):\n",
    "    _, queryEntitiesWithFreq = getQueryEntitiesAndSpots(query, long_text = 0)   ## since query is expected to be short\n",
    "    docEntitiesWithFreq = retrieveEntities(docID, method = method)\n",
    "    docEntityFrequencies = [entityTuple[1] for entityTuple in docEntitiesWithFreq]\n",
    "    queryEntityFrequencies = [entityTuple[1] for entityTuple in queryEntitiesWithFreq]\n",
    "\n",
    "    queryEntities = []\n",
    "    for entityTuple in queryEntitiesWithFreq:\n",
    "        queryEntities.append(preprocess(entityTuple[0]))\n",
    "    queryEntityEmbeddings = embed(queryEntities)\n",
    "    docEntityEmbeddings = [retrieveEntityEmbedding(entityTuple[0]) for entityTuple in docEntitiesWithFreq]\n",
    "    \n",
    "    numDocEntities = len(docEntitiesWithFreq)\n",
    "    numQueryEntities = len(queryEntitiesWithFreq)\n",
    "    simMatrix = np.zeros((numDocEntities, numQueryEntities))\n",
    "    for i in range(numDocEntities):\n",
    "        for j in range(numQueryEntities):\n",
    "            simMatrix[i][j] = max(0, cosineSimilarity(docEntityEmbeddings[i], queryEntityEmbeddings[j]))\n",
    "    return simMatrix,  queryEntityFrequencies, docEntityFrequencies\n",
    "\n",
    "def reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = 'column', pooling = 'max'):\n",
    "    if axis == 'column':\n",
    "        axis = 0\n",
    "    else:\n",
    "        axis = 1\n",
    "    if pooling == 'max':\n",
    "        try:\n",
    "            return np.max(simMatrix, axis = axis) # along columns\n",
    "        except:\n",
    "            return np.zeros(1)\n",
    "    \n",
    "def reduceVector(vector, reduction = 'avg'):\n",
    "    if reduction == 'avg':\n",
    "        return sum(vector) / len(vector)\n",
    "    \n",
    "def semanticScore(query, docID, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simMatrix,  queryEntityFrequencies, docEntityFrequencies = computeSimilarityMatrix(query, docID, method = method)\n",
    "    vector = reduceMatrix(simMatrix,  queryEntityFrequencies, docEntityFrequencies, axis = axis, pooling = pooling)\n",
    "    score = reduceVector(vector, reduction = reduction)\n",
    "    return score\n",
    "\n",
    "\n",
    "def search(query, docIDList, K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'):\n",
    "    simScores = [ semanticScore(query, docID, method = method, axis = axis, pooling = pooling, reduction = reduction) for docID in docIDList]\n",
    "    IDsWithScore = [(score, ID) for score, ID in zip(simScores, docIDList)]\n",
    "   \n",
    "    IDsWithScore.sort(reverse=True)\n",
    "    IDsWithScore = IDsWithScore[:K]                    ## Keep top-K documents only\n",
    "    \n",
    "    return [ID for _,ID in IDsWithScore]\n",
    "\n",
    "\n",
    "def normalize(lis):\n",
    "    _min = min(lis)\n",
    "    _max = max(lis)\n",
    "    lis  = [(x - _min)/(_max - _min) for x in lis]\n",
    "    return lis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9687533aa094ceea0157fb71f9c2018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results1 = []\n",
    "for i in tqdm(range(len(queryList))):\n",
    "    results1.append(search(queryList[i], esResults[i], K = 10, method = 'title', axis = 'column', pooling = 'max', reduction = 'avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store search results to file\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/entity_search_resultsSiamese.json', 'w') as outfile:\n",
    "\n",
    "    outDict = dict()\n",
    "    outDict['result'] = results1\n",
    "    json.dump(outDict, outfile)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f7d079638f46af8bb6c584ccf43e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "IDList = []\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2_doc.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line)\n",
    "        titles.append(data['title'][0])\n",
    "        IDList.append(data['docno'])\n",
    "def ret(paperID):\n",
    "    for id, title in zip(IDList, titles):\n",
    "        if (id == paperID):\n",
    "            return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives'],\n",
       " ['New types of deep neural network learning for speech recognition and related applications: an overview'],\n",
       " ['Deep Learning Architectures for Soil Property Prediction'],\n",
       " ['Privacy-Preserving Deep Learning'],\n",
       " ['Learning Hierarchical Representations for Video Analysis Using Deep Learning'],\n",
       " ['Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey'],\n",
       " ['Evaluation of Deep Learning based Pose Estimation for Sign Language'],\n",
       " ['Comparing Time and Frequency Domain for Audio Event Recognition Using Deep Learning'],\n",
       " ['Deep Learning using Support Vector Machines'],\n",
       " ['Kernel Methods for Deep Learning']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(id) for id   in results1[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deep learning',\n",
       " 'artificial intelligence',\n",
       " 'information retrieval',\n",
       " 'machine learning',\n",
       " 'question answering',\n",
       " 'noun phrases',\n",
       " 'penn treebank',\n",
       " 'speech recognition',\n",
       " 'data mining',\n",
       " 'computer vision']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paperIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deep learning',\n",
       " 'artificial intelligence',\n",
       " 'information retrieval',\n",
       " 'machine learning',\n",
       " 'question answering',\n",
       " 'noun phrases',\n",
       " 'penn treebank',\n",
       " 'speech recognition',\n",
       " 'data mining',\n",
       " 'computer vision']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevanceScores, k = 10, method=0):\n",
    "    \"\"\"\n",
    "    Returns discounted cumulative gain (dcg)\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    relevanceScores = np.asfarray(relevanceScores)[:k]\n",
    "    if relevanceScores.size:\n",
    "        if method == 0:\n",
    "            return relevanceScores[0] + np.sum(relevanceScores[1:] / np.log2(np.arange(2, relevanceScores.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0\n",
    "\n",
    "def ndcgMax(relevanceScores, k=10, method=0):\n",
    "    return dcg(sorted(relevanceScores, reverse=True), k, method)\n",
    "\n",
    "def ndcg(relevanceScores, ndcgMax, k = 10, method=0):\n",
    "    return dcg(relevanceScores, k, method) / ndcgMax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryToIdx = {queryList[i]:i for i in range(len(queryList))}\n",
    "annotationDict = [{} for i in range(len(queryList) + 1)]    # + 1 for 1 based indexing in\n",
    "with open('./data/Explicit_Semantic_Ranking_Dataset/s2.qrel') as file:\n",
    "    for line in file:\n",
    "        lineString = line.split()\n",
    "        qid = int(lineString[0])\n",
    "        if qid > 10:\n",
    "            break\n",
    "        docno = lineString[2]\n",
    "        relScore = int(lineString[-1])\n",
    "        annotationDict[qid][docno] = relScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevanceScores= []\n",
    "\n",
    "fileName = './data/Explicit_Semantic_Ranking_Dataset/entity_search_resultsSiamese.json'\n",
    "with open(fileName, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        result = data['result']\n",
    "        for i in range(len(result)):\n",
    "            result[i] = [annotationDict[i + 1].get(ID, 0) for ID in result[i]]\n",
    "        relevanceScores.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 2],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [2, 0, 0, 0, 0, 0, 4, 0, 0, 4],\n",
       "  [0, 3, 2, 0, 0, 0, 0, 0, 0, 0]]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevanceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tuning Metaheuristics - A Machine Learning Perspective'],\n",
       " ['Supervised Machine Learning: A Review of Classification Techniques'],\n",
       " ['Machine Learning, Neural and Statistical Classification'],\n",
       " ['Introduction to machine learning for brain imaging'],\n",
       " ['Machine Learning - An Algorithmic Perspective'],\n",
       " ['Bioinformatics - the machine learning approach'],\n",
       " ['C4.5: Programs for Machine Learning'],\n",
       " ['Introduction to Machine Learning Introduction to Machine Learning Introduction to Machine Learning Introduction to Machine Learning Introduction to Machine Learning'],\n",
       " ['Softprop: Softmax Neural Network Backpropagation Learning'],\n",
       " ['Information Geometry and Information Theory in Machine Learning']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(ID) for ID in results1[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'227759bc318163b2f2490690b828263f3f020cfb': 2,\n",
       " '373f76633cc1f6c7a421e31c989842021a52fca4': 4,\n",
       " '72d32c986b47d6b880dad0c3f155fe23d2939038': 3,\n",
       " '39f63dbdce9207b87878290c0e3983e84cfcecd9': 1,\n",
       " '5ca4abab527f6b0270e50548f0dea30638c9b86e': 0,\n",
       " '013cd20c0eaffb9cab80875a43086e0c3224fe20': 2,\n",
       " '2c03df8b48bf3fa39054345bafabfeff15bfd11d': 3,\n",
       " '12d1d070a53d4084d88a77b8b143bad51c40c38f': 0,\n",
       " '4728bac8f82149c844c50045fd62c550622b7a01': 0,\n",
       " '5352b7ca90cbe4938f8e71a25d49517e7f94670a': 0,\n",
       " '76e282712f35424d160d801a72e48372ab891a50': 0,\n",
       " 'ad8c2721ef54c9326684762db7c9fc1378e83797': 0,\n",
       " 'a62b58c267fddfa06545a7fc63a3c62ef7dc9e15': 0,\n",
       " '1d7705be75f4e29210373c2b40ee5cb6e46f0007': 0,\n",
       " '940b84cae7f8cef27351c7e0ff472cc3a80aff8c': 0,\n",
       " '1c40786dc5cc14efeb3a08e08bfdfdc52995b3ea': 0,\n",
       " '2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc': 4,\n",
       " '0a8149fb5aa8a5684e7d530c264451a5cb9250f5': 0,\n",
       " '22ce15125f1c8fb466f241ce010e670d23dcc764': 0,\n",
       " 'd26a48aff2abc3460c1018d5b410766f698d696c': 0,\n",
       " '553a6530b0802da9bec354d0a70fde254f6a5e36': 0,\n",
       " '1721f5ee35869512f46035833ad1b6ad6346b2ad': 0,\n",
       " '0d30860edf7dd5362436e6fd5262c618e33573d5': 1,\n",
       " '1a07186bc10592f0330655519ad91652125cd907': 1,\n",
       " '27208c88f07a1ffe97760c12be08fad3ab68fee2': 0,\n",
       " '0d67362a5630ec3b7562327acc278c1c996454b5': 4,\n",
       " '16243557482241171beccbbd694976103cc941ef': 0,\n",
       " 'c72f5e1fa2eaa10933f0ffac95d323a80f50b00a': 0,\n",
       " '47225c992d7086cf5d113942212edb4a57401130': 0,\n",
       " '4149af3e0745ee805187eb1b8b65d7f8ced615ea': 1,\n",
       " '55136e0b6378e1b9c2738e3da6bdcf759c400dd3': 0,\n",
       " '051f689825d4f118a39a286cf72888d2d1a84438': 0,\n",
       " '4c0e637fecc62b689f78af4e934bb85cdc09f048': 0,\n",
       " 'fcb42c401c36606216560118b4003dcd67bd5740': 0,\n",
       " '558c7e0fec15fee4de5d3a768725f9128d3f2002': 0,\n",
       " 'e2b7f37cd97a7907b1b8a41138721ed06a0b76cd': 1,\n",
       " 'd2d6451d76d1ec5cfb667df324c43872335f67a0': 0,\n",
       " '0122e063ca5f0f9fb9d144d44d41421503252010': 1}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotationDict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations'],\n",
       "  2),\n",
       " (['A Fast Learning Algorithm for Deep Belief Nets'], 4),\n",
       " (['Deep Learning of Representations: Looking Forward'], 3),\n",
       " (['Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition'],\n",
       "  1),\n",
       " (['Exact solutions to the nonlinear dynamics of learning in deep linear neural networks'],\n",
       "  0),\n",
       " (['Representation Learning: A Review and New Perspectives'], 2),\n",
       " (['Deep Residual Learning for Image Recognition'], 3),\n",
       " (['Reinforcement Learning: A Survey'], 0),\n",
       " (['Building high-level features using large scale unsupervised learning'], 0),\n",
       " (['Scalable stacking and learning for building deep architectures'], 0),\n",
       " (None, 0),\n",
       " (['Unsupervised learning of hierarchical representations with convolutional deep belief networks'],\n",
       "  0),\n",
       " (None, 0),\n",
       " (['Deep learning in speech synthesis'], 0),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (['ImageNet Classification with Deep Convolutional Neural Networks'], 4),\n",
       " (['Recent Advances in Hierarchical Reinforcement Learning'], 0),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (['Knowledge-Powered Deep Learning for Word Embedding'], 0),\n",
       " (None, 0),\n",
       " (['Multimodal learning with deep Boltzmann machines'], 1),\n",
       " (['A unified architecture for natural language processing: deep neural networks with multitask learning'],\n",
       "  1),\n",
       " (['Multimodal Deep Learning'], 0),\n",
       " (['Learning Deep Architectures for AI'], 4),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (['Why Does Unsupervised Pre-training Help Deep Learning?'], 1),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (['Continuous Learning of Human Activity Models Using Deep Nets'], 0),\n",
       " (None, 0),\n",
       " (None, 0),\n",
       " (['Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion'],\n",
       "  1),\n",
       " (None, 0),\n",
       " (['Large Scale Distributed Deep Networks'], 1)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ret(ID), annotationDict[1][ID]) for ID in annotationDict[1].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgMaxPerQuery = []\n",
    "for i in range(len(queryList)):\n",
    "    ndcgMaxPerQuery.append(ndcgMax(list(annotationDict[i].values())))\n",
    "ndcgMaxPerQuery[-1] = ndcgMaxPerQuery[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 15.75169688297418,\n",
       " 6.561606311644851,\n",
       " 17.40255958161545,\n",
       " 11.323465818787765,\n",
       " 8.385424265341916,\n",
       " 12.69009057132933,\n",
       " 15.070595335185766,\n",
       " 17.146988662861663,\n",
       " 0]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcgMaxPerQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.01797804708183"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcgMax([4] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Survey on speech emotion recognition: Features, classification schemes, and databases'],\n",
       " ['Efficient voice activity detection algorithms using long-term speech information'],\n",
       " ['The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions'],\n",
       " ['Normalized amplitude modulation features for large vocabulary noise-robust speech recognition'],\n",
       " ['A coupled HMM for audio-visual speech recognition'],\n",
       " ['Speech production knowledge in automatic speech recognition.'],\n",
       " ['Convolutional Neural Networks for Speech Recognition'],\n",
       " ['Audio-visual continuous speech recognition using a coupled hidden Markov model'],\n",
       " ['Recent Development of Open-source Speech Recognition Engine Julius'],\n",
       " ['Speaker independent audio-visual continuous speech recognition']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ret(ID) for ID in esResults[7][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
